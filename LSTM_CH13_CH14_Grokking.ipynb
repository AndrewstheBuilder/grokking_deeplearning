{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhpTnrnumdTVgW15xdy+Sw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewstheBuilder/grokking_deeplearning/blob/main/LSTM_CH13_CH14_Grokking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grokking Deep Learning Framework Begin"
      ],
      "metadata": {
        "id": "LnwAeGaopjiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Tensor (object):\n",
        "  def __init__(self,data,label='None',\n",
        "               autograd=False,\n",
        "               creators=None,\n",
        "               creation_op=None,\n",
        "               id=None):\n",
        "    # print('label',label)\n",
        "    self.label = label\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "    if(id is None):\n",
        "      id = np.random.randint(0,100000) # What is the likelyhood of producing the same id for a tensor in the same session?\n",
        "    self.id = id\n",
        "\n",
        "    if(creators is not None):\n",
        "      # print('creators',creators)\n",
        "      for c in creators:\n",
        "        # keeps track of how many children a tensor has\n",
        "        if(self.id not in c.children):\n",
        "          # Initialize c.children[self.id]\n",
        "          # We are giving the creator the children property\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          # Update counter for children\n",
        "          # What is this scenario ???\n",
        "          # Each child should have 2 separate parents and that's it\n",
        "          # Another way of saying this ^ is that all parents have one child\n",
        "          print('when does this get called')\n",
        "          print('c',c)\n",
        "          print('self',self)\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    '''\n",
        "    Checks whether a tensor has received the correct\n",
        "    number of gradients from each child\n",
        "    '''\n",
        "    for id,cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  def backward(self,grad=None,grad_origin=None):\n",
        "    print('self',self)\n",
        "    print('grad',grad)\n",
        "    print('grad_origin',grad_origin)\n",
        "    print()\n",
        "    if(self.autograd):\n",
        "      if(grad_origin is not None):\n",
        "        if(self.children[grad_origin.id] == 0):\n",
        "          raise Exception(\"cannot backprop more than once\")\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      if(self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        # accumulates gradients from several children\n",
        "        self.grad += grad\n",
        "\n",
        "      if(self.creators is not None and\n",
        "         (self.all_children_grads_accounted_for() or\n",
        "          grad_origin is None)):\n",
        "          # begins actual back propagation\n",
        "          if(self.creation_op == \"add\"):\n",
        "            self.creators[0].backward(grad,self)\n",
        "            self.creators[1].backward(grad,self)\n",
        "\n",
        "          if(self.creation_op == \"neg\"):\n",
        "            self.creators[0].backward(self.grad.__neg__())\n",
        "    # old code before adding support for multiuse tensors\n",
        "    # self.grad = grad\n",
        "\n",
        "    # if(self.creation_op == \"add\"):\n",
        "    #   self.creators[0].backward(grad)\n",
        "    #   self.creators[1].backward(grad)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data,\n",
        "                    label=self.label+' + '+other.label,\n",
        "                    autograd = True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"add\")\n",
        "    # print(\"When would this case ever be true?\")\n",
        "    # print('self',self)\n",
        "    # print('other',other)\n",
        "    # This case gets called when we are accumulating the gradients and add the self.grad += grad\n",
        "    # if((self.autograd == False and other.autograd == True) or (self.autograd == True and other.autograd == False)):\n",
        "    #   print('how did this happen??')\n",
        "    #   print('self',self)\n",
        "    #   print('other',other)\n",
        "    return Tensor(self.data + other.data)\n",
        "\n",
        "  def __neg__(self):\n",
        "    if(self.autograd):\n",
        "      # I think this tensor replaces the tensor I have as I initially declared it\n",
        "      # print('neg self',self)\n",
        "      # print('self.data',self.data)\n",
        "      return Tensor(self.data*-1,\n",
        "                    label='-'+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"neg\",)\n",
        "    # print('when does neg this else statement occur', self)\n",
        "    # It happens for the gradient calculation. When we are backpropagating the grad from the child.\n",
        "    return Tensor(self.data*-1)\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data - other.data,\n",
        "                    label = self.label + ' - ' + other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"sub\")\n",
        "    # if((self.autograd == False and other.autograd == True) or (self.autograd == True and other.autograd == False)):\n",
        "    #   print('how did this happen??')\n",
        "    #   print('self',self)\n",
        "    #   print('other',other)\n",
        "\n",
        "    return Tensor(self.data - other.data)\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data * other.data,\n",
        "                    label = self.label+'*'+other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"mul\")\n",
        "    # if((self.autograd == False and other.autograd == True) or (self.autograd == True and other.autograd == False)):\n",
        "    #   print('how did this happen??')\n",
        "    #   print('self',self)\n",
        "    #   print('other',other)\n",
        "\n",
        "    return Tensor(self.data - other.data)\n",
        "\n",
        "  def sum(self, dim):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.sum(dim),\n",
        "                    label = self.label+'.sum_'+str(dim)+')',\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sum_\"+str(dim))\n",
        "    # if((self.autograd == False)):\n",
        "    #   print('how did this happen?? in sum')\n",
        "    #   print('self',self)\n",
        "    #   print('dim',dim)\n",
        "\n",
        "    return Tensor(self.data.sum(dim))\n",
        "\n",
        "  def expand(self, dim, copies):\n",
        "\n",
        "    trans_cmd = list(range(0, len(self.data.shape)))\n",
        "    trans_cmd.insert(dim, len(self.data.shape))\n",
        "    new_shape = list(self.data.shape) + [copies]\n",
        "    new_data = self.data.repeat(copies).reshape(new_shape)\n",
        "    new_data = new_data.transpose(trans_cmd)\n",
        "\n",
        "    if(self.autograd):\n",
        "      return Tensor(new_data,\n",
        "                    label=self.label+\".expand_\"+str(dim),\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"expand_\"+str(dim))\n",
        "    # print('How the heck did you get here in expand')\n",
        "    # print('self',self)\n",
        "    # print('dim',dim)\n",
        "    # print('copies',copies)\n",
        "    return new_data\n",
        "\n",
        "  def transpose(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.transpose(),\n",
        "                    label=self.label+\".transpose\",\n",
        "                    autograd=True,\n",
        "                    creators=self,\n",
        "                    creation_op=\"transpose\")\n",
        "    # print(\"How did you get here in transpose()\")\n",
        "    # print('self',self)\n",
        "    return Tensor(self.data.transpose())\n",
        "\n",
        "  def mm(self,x):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.dot(x.data),\n",
        "                    label=self.label+\".dot_\"+x.data,\n",
        "                    autograd=True,\n",
        "                    creators=self,\n",
        "                    creation_op=\"mm\")\n",
        "    # print(\"How did you get here in mm()\")\n",
        "    # print('self',self)\n",
        "    return Tensor(self.data.dot(x.data))\n",
        "\n",
        "  def __repr__(self):\n",
        "    # This method calls the self.data's repr method\n",
        "    return str(self.label.__repr__() + \":\" + self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.label.__repr__() + \":\" + self.data.__str__() + ' Creators:'+self.creators.__str__())\n",
        "\n",
        "x = Tensor(np.array([[1,2,3],\n",
        "                     [4,5,6]]), label='x')\n",
        "x.sum(0)\n",
        "\n",
        "# a = Tensor(([1,2,3,4,5]), label='a', autograd=True)\n",
        "# b = Tensor(([2,2,2,2,2]), label='b', autograd=True)\n",
        "# c = Tensor(([5,4,3,2,1]), label='c', autograd=True)\n",
        "\n",
        "# d = a + (-b)\n",
        "# e = (-b) + c\n",
        "# f = d + e\n",
        "# f.backward(Tensor(np.array([1,1,1,1,1]), label='initial grad'))\n",
        "\n",
        "# # f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "\n",
        "# print('b.grad.data',b.grad.data)\n",
        "# d = a + b\n",
        "# e = b + c\n",
        "# f = d + e\n",
        "\n",
        "# f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "# # f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "# print(b.grad.data == np.array([2,2,2,2,2]))\n",
        "# print('f.grad',f.grad)\n",
        "# print('e.grad',e.grad)\n",
        "# print('d.grad',d.grad)\n",
        "# print('b.grad',b.grad)\n",
        "# Old code before adding support for multiuse tensors\n",
        "# x = Tensor([1,2,3,4,5])\n",
        "# y = Tensor([2,2,2,2,2])\n",
        "\n",
        "# z = x+y\n",
        "# z.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "# print(x.grad)\n",
        "# print(y.grad)\n",
        "# print(z.creators)\n",
        "# print(z.creation_op)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUmIEoc-pnhi",
        "outputId": "e4a41c4e-6918-48b8-f763-b5ea8ddc1495"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'None':array([5, 7, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A toy example of RNN Backpropagation with exploding and vanishing gradients"
      ],
      "metadata": {
        "id": "Kepf5QuJfVvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "RcJTWeu_f285"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sigmoid,relu) = (lambda x: 1/1+np.exp(-x)), lambda x: (x>0).astype(float)*x\n",
        "weights = np.array([[1,4],[4,1]])\n",
        "activation = sigmoid(np.array([1, 0.01]))\n",
        "\n",
        "print(\"Sigmoid Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  activation = sigmoid(activation.dot(weights))\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\nSigmoid Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  # The derivative of sigmoid causes very small gradients when activation is very near 0 or 1\n",
        "  sigmoid_deriv = (activation) * (1-activation)\n",
        "   # Chain Rule\n",
        "  gradient = sigmoid_deriv * gradient\n",
        "  gradient = gradient.dot(weights.transpose()) # So this is also part of the chain rule???\n",
        "  print(gradient)\n",
        "\n",
        "print(\"\\nRelu Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  # The matrix multiplication causes exploding gradients that don't get squashed by a nonlinearity as in sigmoid\n",
        "  activation = relu(activation.dot(weights))\n",
        "\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\n Relu Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
        "  print(gradient)\n",
        "\n",
        "# Adding gates to RNN will replace all of the nonlinearies and matrix multiplications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ01X-TCfaVg",
        "outputId": "f54cbb9e-8696-423f-9af0-e8fb4bc85180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Activations\n",
            "[1.00008889 1.00057475]\n",
            "[1.00672188 1.00673168]\n",
            "[1.006515   1.00651519]\n",
            "[1.00652199 1.00652199]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "\n",
            "Sigmoid Gradients\n",
            "[-0.03282154 -0.03282154]\n",
            "[0.00107725 0.00107725]\n",
            "[-3.53571093e-05 -3.53571093e-05]\n",
            "[1.16047468e-06 1.16047468e-06]\n",
            "[-3.80885641e-08 -3.80885641e-08]\n",
            "[1.25012386e-09 1.25012385e-09]\n",
            "[-4.10323732e-11 -4.10323591e-11]\n",
            "[1.34536847e-12 1.34534485e-12]\n",
            "[-4.55737812e-14 -4.55341556e-14]\n",
            "[1.08795551e-16 4.23921767e-17]\n",
            "\n",
            "Relu Activations\n",
            "[5.00238791 5.00093033]\n",
            "[25.00610921 25.01048197]\n",
            "[125.04803709 125.03491883]\n",
            "[625.1877124  625.22706719]\n",
            "[3126.09598115 3125.97791678]\n",
            "[15630.00764826 15630.36184138]\n",
            "[78151.45501378 78150.39243441]\n",
            "[390753.02475143 390756.21248955]\n",
            "[1953777.87470964 1953768.31149529]\n",
            "[9768851.12069078 9768879.81033384]\n",
            "\n",
            " Relu Gradients\n",
            "[5. 5.]\n",
            "[25. 25.]\n",
            "[125. 125.]\n",
            "[625. 625.]\n",
            "[3125. 3125.]\n",
            "[15625. 15625.]\n",
            "[78125. 78125.]\n",
            "[390625. 390625.]\n",
            "[1953125. 1953125.]\n",
            "[9765625. 9765625.]\n"
          ]
        }
      ]
    }
  ]
}