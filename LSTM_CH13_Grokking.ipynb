{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbdCK8IP01Es72Enktu6+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewstheBuilder/grokking_deeplearning/blob/main/LSTM_CH13_Grokking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grokking Deep Learning Framework Begin"
      ],
      "metadata": {
        "id": "LnwAeGaopjiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor Object. This lays the groundwork for the autograd engine"
      ],
      "metadata": {
        "id": "_0WE7oAKYRLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Tensor (object):\n",
        "  def __init__(self,data,label='None',\n",
        "               autograd=False,\n",
        "               creators=None,\n",
        "               creation_op=None,\n",
        "               id=None):\n",
        "    # print('label',label)\n",
        "    self.label = label\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "    if(id is None):\n",
        "      id = np.random.randint(0,100000) # What is the likelyhood of producing the same id for a tensor in the same session?\n",
        "    self.id = id\n",
        "\n",
        "    if(creators is not None):\n",
        "      # print('creators',creators)\n",
        "      for c in creators:\n",
        "        # keeps track of how many children a tensor has\n",
        "        if(self.id not in c.children):\n",
        "          # Initialize c.children[self.id]\n",
        "          # We are giving the creator the children property\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          # Update counter for children\n",
        "          # What is this scenario ???\n",
        "          # Each child should have 2 separate parents and that's it\n",
        "          # Another way of saying this ^ is that all parents have one child\n",
        "          print('when does this get called')\n",
        "          print('c',c)\n",
        "          print('self',self)\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    '''\n",
        "    Checks whether a tensor has received the correct\n",
        "    number of gradients from each child\n",
        "    '''\n",
        "    for id,cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  def backward(self,grad=None,grad_origin=None):\n",
        "    # Back Prop logging\n",
        "    # Might want to enable this for future?\n",
        "    # print('self',self)\n",
        "    # print('grad',grad)\n",
        "    # print('grad_origin',grad_origin)\n",
        "    # print()\n",
        "    if(self.autograd):\n",
        "\n",
        "      if(grad is None):\n",
        "        grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "      if(grad_origin is not None):\n",
        "        if(self.children[grad_origin.id] == 0):\n",
        "          raise Exception(\"cannot backprop more than once\")\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      if(self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        # accumulates gradients from several children\n",
        "        self.grad += grad\n",
        "\n",
        "      if(self.creators is not None and\n",
        "         (self.all_children_grads_accounted_for() or\n",
        "          grad_origin is None)):\n",
        "          # begins actual back propagation\n",
        "          if(self.creation_op == \"add\"):\n",
        "            self.creators[0].backward(grad,self)\n",
        "            self.creators[1].backward(grad,self)\n",
        "\n",
        "          if(self.creation_op == \"neg\"):\n",
        "            self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "          if(self.creation_op == \"sub\"):\n",
        "            new = Tensor(self.grad.data, label='sub_grad')\n",
        "            self.creators[0].backward(new, self)\n",
        "            new = Tensor(self.grad.__neg__().data, label='sub_grad2')\n",
        "            self.creators[1].backward(new,self)\n",
        "\n",
        "          if(self.creation_op == \"mul\"):\n",
        "            new = self.grad * self.creators[1].data\n",
        "            self.creators[0].backward(new, self)\n",
        "            new = self.grad * self.creators[0].data\n",
        "            self.creators[1].backward(new, self)\n",
        "\n",
        "          if(self.creation_op == \"mm\"):\n",
        "            # Usually an activation\n",
        "            act = self.creators[0]\n",
        "            # print('act in mm() backward', act)\n",
        "            # print('self.creators',self.creators)\n",
        "            # Usually a weight matrix\n",
        "            weights = self.creators[1]\n",
        "            new = self.grad.mm(weights.transpose())\n",
        "            act.backward(new)\n",
        "            new = self.grad.transpose().mm(act).transpose()\n",
        "            weights.backward(new)\n",
        "\n",
        "          if(self.creation_op == \"transpose\"):\n",
        "            self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "          if(\"sum\" in self.creation_op):\n",
        "            dim = int(self.creation_op.split(\"_\")[1])\n",
        "            # print('dim in sum backward',dim)\n",
        "            # print('ds',ds)\n",
        "            ds = self.creators[0].data.shape[dim]\n",
        "            self.creators[0].backward(self.grad.expand(dim, ds))\n",
        "\n",
        "          if(\"expand\" in self.creation_op):\n",
        "            dim = int(self.creation_op.split(\"_\")[1])\n",
        "            self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "          if(self.creation_op == \"sigmoid\"):\n",
        "            ones = Tensor(np.ones_like(self.grad.data))\n",
        "            self.creators[0].backward(Tensor(self.grad.data * (self.data * (ones.data - self.data))))\n",
        "\n",
        "          if(self.creation_op == \"tanh\"):\n",
        "            ones = Tensor(np.ones_like(self.grad.data))\n",
        "            self.creators[0].backward(Tensor(self.grad.data * (ones.data - self.data)))\n",
        "\n",
        "    # old code before adding support for multiuse tensors\n",
        "    # self.grad = grad\n",
        "\n",
        "    # if(self.creation_op == \"add\"):\n",
        "    #   self.creators[0].backward(grad)\n",
        "    #   self.creators[1].backward(grad)\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data,\n",
        "                    label=self.label+' + '+other.label,\n",
        "                    autograd = True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"add\")\n",
        "    # print(\"When would this case ever be true?\")\n",
        "    # print('self',self)\n",
        "    # print('other',other)\n",
        "    # This case gets called when we are accumulating the gradients and add the self.grad += grad\n",
        "    # if((self.autograd == False and other.autograd == True) or (self.autograd == True and other.autograd == False)):\n",
        "    #   print('how did this happen??')\n",
        "    #   print('self',self)\n",
        "    #   print('other',other)\n",
        "    return Tensor(self.data + other.data)\n",
        "\n",
        "  def __neg__(self):\n",
        "    if(self.autograd):\n",
        "      # I think this tensor replaces the tensor I have as I initially declared it\n",
        "      # print('neg self',self)\n",
        "      # print('self.data',self.data)\n",
        "      return Tensor(self.data*-1,\n",
        "                    label='-'+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"neg\",)\n",
        "    # print('when does neg this else statement occur', self)\n",
        "    # It happens for the gradient calculation. When we are backpropagating the grad from the child.\n",
        "    return Tensor(self.data*-1)\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data - other.data,\n",
        "                    label = self.label + ' - ' + other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"sub\")\n",
        "    # if((self.autograd == False and other.autograd == True) or (self.autograd == True and other.autograd == False)):\n",
        "    #   print('how did this happen??')\n",
        "    #   print('self',self)\n",
        "    #   print('other',other)\n",
        "\n",
        "    return Tensor(self.data - other.data)\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data * other.data,\n",
        "                    label = self.label+'*'+other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"mul\")\n",
        "    # if((self.autograd == False and other.autograd == True) or (self.autograd == True and other.autograd == False)):\n",
        "    #   print('how did this happen??')\n",
        "    #   print('self',self)\n",
        "    #   print('other',other)\n",
        "\n",
        "    return Tensor(self.data - other.data)\n",
        "\n",
        "  def sum(self, dim):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.sum(dim),\n",
        "                    label = self.label+'.sum_'+str(dim)+')',\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sum_\"+str(dim))\n",
        "    # if((self.autograd == False)):\n",
        "    #   print('how did this happen?? in sum')\n",
        "    #   print('self',self)\n",
        "    #   print('dim',dim)\n",
        "\n",
        "    return Tensor(self.data.sum(dim))\n",
        "\n",
        "  def expand(self, dim, copies):\n",
        "\n",
        "    trans_cmd = list(range(0, len(self.data.shape)))\n",
        "    trans_cmd.insert(dim, len(self.data.shape))\n",
        "    new_shape = list(self.data.shape) + [copies]\n",
        "    new_data = self.data.repeat(copies).reshape(new_shape)\n",
        "    new_data = new_data.transpose(trans_cmd)\n",
        "\n",
        "    if(self.autograd):\n",
        "      return Tensor(new_data,\n",
        "                    label=self.label+\".expand_\"+str(dim),\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"expand_\"+str(dim))\n",
        "    # print('How the heck did you get here in expand')\n",
        "    # print('self',self)\n",
        "    # print('dim',dim)\n",
        "    # print('copies',copies)\n",
        "    return new_data\n",
        "\n",
        "  def transpose(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.transpose(),\n",
        "                    label=self.label+\".transpose\",\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"transpose\")\n",
        "    # print(\"How did you get here in transpose()\")\n",
        "    # print('self',self)\n",
        "    return Tensor(self.data.transpose())\n",
        "\n",
        "  def mm(self,x):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.dot(x.data),\n",
        "                    label=self.label+\".dot_\"+x.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self,x],\n",
        "                    creation_op=\"mm\")\n",
        "    # print(\"How did you get here in mm()\")\n",
        "    # print('self',self)\n",
        "    return Tensor(self.data.dot(x.data))\n",
        "\n",
        "  # Nonlinearities\n",
        "  def sigmoid(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(1/(1+np.exp(-self.data)),\n",
        "                    label=\"sigmoid_\"+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sigmoid\")\n",
        "    return Tensor(1/(1+np.exp(-self.data)), label=\"(no auto grad)sigmoid_\"+self.label)\n",
        "\n",
        "  def tanh(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(np.tanh(self.data),\n",
        "                    label=\"tanh_\"+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"tanh\")\n",
        "    return Tensor(np.tanh(self.data), label=\"(no auto grad)tanh\"+self.label)\n",
        "\n",
        "  def __repr__(self):\n",
        "    # This method calls the self.data's repr method\n",
        "    return str(self.data.__repr__())\n",
        "    # return str(self.label.__repr__() + \":\" + self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())\n",
        "    # return str(self.label.__repr__() + \":\" + self.data.__str__() + ' Creators:'+self.creators.__str__())\n",
        "\n",
        "x = Tensor(np.array([[1,2,3],\n",
        "                     [4,5,6]]), label='x')\n",
        "y = x.sum(0)\n",
        "y.backward()\n",
        "# x.expand(dim=0, copies=1)\n",
        "\n",
        "\n",
        "# a = Tensor(([1,2,3,4,5]), label='a', autograd=True)\n",
        "# b = Tensor(([2,2,2,2,2]), label='b', autograd=True)\n",
        "# c = Tensor(([5,4,3,2,1]), label='c', autograd=True)\n",
        "\n",
        "# d = a + (-b)\n",
        "# e = (-b) + c\n",
        "# f = d + e\n",
        "# f.backward(Tensor(np.array([1,1,1,1,1]), label='initial grad'))\n",
        "\n",
        "# # f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "\n",
        "# print('b.grad.data',b.grad.data)\n",
        "# d = a + b\n",
        "# e = b + c\n",
        "# f = d + e\n",
        "\n",
        "# f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "# # f.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "# print(b.grad.data == np.array([2,2,2,2,2]))\n",
        "# print('f.grad',f.grad)\n",
        "# print('e.grad',e.grad)\n",
        "# print('d.grad',d.grad)\n",
        "# print('b.grad',b.grad)\n",
        "# Old code before adding support for multiuse tensors\n",
        "# x = Tensor([1,2,3,4,5])\n",
        "# y = Tensor([2,2,2,2,2])\n",
        "\n",
        "# z = x+y\n",
        "# z.backward(Tensor(np.array([1,1,1,1,1])))\n",
        "# print(x.grad)\n",
        "# print(y.grad)\n",
        "# print(z.creators)\n",
        "# print(z.creation_op)"
      ],
      "metadata": {
        "id": "ZUmIEoc-pnhi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent Class"
      ],
      "metadata": {
        "id": "4RkhZ2_FYbQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(object):\n",
        "  def __init__(self, parameters, alpha=0.1):\n",
        "    self.parameters = parameters\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def zero(self):\n",
        "    # zero parameters' gradients\n",
        "    for p in self.parameters:\n",
        "      p.grad.data *= 0\n",
        "\n",
        "  def step(self, zero=True):\n",
        "    # update parameters' data based on their gradients\n",
        "    # zero out the gradient after if zero=True\n",
        "    for p in self.parameters:\n",
        "      p.data -= p.grad.data * self.alpha\n",
        "      if(zero):\n",
        "        p.grad.data *= 0"
      ],
      "metadata": {
        "id": "QXRl_6cA4U1N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toy Example that does not use the autograd system"
      ],
      "metadata": {
        "id": "FdnnrIqrmGxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Abstract Class(Makes sure parameters are stored in the subclass because that is what a layer represents) + Linear Layer Subclass Implementing Layer"
      ],
      "metadata": {
        "id": "nZGnQeURYeCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.parameters = list()\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    super().__init__()\n",
        "    W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs))\n",
        "    self.weight = Tensor(W, autograd=True)\n",
        "    self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "\n",
        "    self.parameters.append(self.weight)\n",
        "    self.parameters.append(self.bias)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.mm(self.weight)+self.bias.expand(0, len(input.data))"
      ],
      "metadata": {
        "id": "gst79s2T6q_u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential Object that is a subclass of Layer. It will probably be used to stack a bunch of other layer subclass objects on top of each other."
      ],
      "metadata": {
        "id": "y7WWOA-kYmfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "class Sequential(Layer):\n",
        "\n",
        "  def __init__(self, layers=list()):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def forward(self, input):\n",
        "    for layer in self.layers:\n",
        "      input = layer.forward(input)\n",
        "    return input\n",
        "\n",
        "  def get_parameters(self):\n",
        "    params = list()\n",
        "    for l in self.layers:\n",
        "      params += l.get_parameters()\n",
        "    return params\n",
        "\n",
        "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
        "target = Tensor(np.array([[0], [1], [0], [1]]), autograd=True)\n",
        "\n",
        "model = Sequential([Linear(2,3), Linear(3,1)])\n",
        "\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
        "\n",
        "for i in range(10):\n",
        "  # predict\n",
        "  pred = model.forward(data)\n",
        "\n",
        "  # Compare Mean Squared Error\n",
        "  loss = ((pred-target) * (pred-target)).sum(0)\n",
        "\n",
        "  # Learn\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ycQU13E9dl9",
        "outputId": "5e38be85-d108-4218-c691-b3787573e8a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.33428272]\n",
            "[0.62282083]\n",
            "[0.19680451]\n",
            "[0.08915535]\n",
            "[0.06028456]\n",
            "[0.049625]\n",
            "[0.04329267]\n",
            "[0.03828787]\n",
            "[0.0339512]\n",
            "[0.03010911]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layers without weights. Here we got the Mean Squared Error Function. Why do we extend the Layer class in MSELoss"
      ],
      "metadata": {
        "id": "XfV4aM71-39x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also create layers that are functions on the input\n",
        "class MSELoss(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, pred, target):\n",
        "    return ((pred-target)*(pred-target)).sum(0)\n",
        "\n",
        "import numpy\n",
        "np.random.seed(0)\n",
        "\n",
        "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
        "target = Tensor(np.array([[0], [1], [0], [1]]), autograd=True)\n",
        "\n",
        "model = Sequential([Linear(2,3), Linear(3,1)])\n",
        "criterion = MSELoss()\n",
        "\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
        "\n",
        "for i in range(10):\n",
        "  # predict\n",
        "  pred = model.forward(data)\n",
        "\n",
        "  # Compare\n",
        "  # Using Mean Squared Error\n",
        "  loss = criterion.forward(pred, target)\n",
        "\n",
        "  # Learn\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ2_9I3X-0I5",
        "outputId": "ec3db822-daea-47b1-dd0b-8327c4a4c954"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.33428272]\n",
            "[0.62282083]\n",
            "[0.19680451]\n",
            "[0.08915535]\n",
            "[0.06028456]\n",
            "[0.049625]\n",
            "[0.04329267]\n",
            "[0.03828787]\n",
            "[0.0339512]\n",
            "[0.03010911]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonlinearities"
      ],
      "metadata": {
        "id": "V2hjPNsiZsvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.tanh()\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.sigmoid()\n",
        "\n",
        "# Trying out the new non linearities 04/03/24. This should be the latest testing of the autograd framework.\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
        "target = Tensor(np.array([[0], [1], [0], [1]]), autograd=True)\n",
        "\n",
        "model = Sequential([Linear(2,3), Tanh(), Linear(3,1), Sigmoid()])\n",
        "criterion = MSELoss()\n",
        "\n",
        "\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
        "\n",
        "for i in range(10):\n",
        "  # predict\n",
        "  pred = model.forward(data)\n",
        "\n",
        "  # Compare Mean Squared Error\n",
        "  loss = criterion.forward(pred, target)\n",
        "\n",
        "  # Learn\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_CBeWYFZrif",
        "outputId": "a1cd414c-024f-48c8-cb1a-d11860202651"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.06372865]\n",
            "[0.75475912]\n",
            "[0.6013857]\n",
            "[0.44200419]\n",
            "[0.29939641]\n",
            "[0.19018209]\n",
            "[0.12855584]\n",
            "[0.08755327]\n",
            "[0.066503]\n",
            "[0.0518286]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "np.random.seed(0)\n",
        "\n",
        "data = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "target = np.array([[0], [1], [0], [1]])\n",
        "\n",
        "weights_0_1 = np.random.rand(2,3)\n",
        "weights_1_2 = np.random.rand(3,1)\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "  # These 'layer' variables are intermediate variables that won't be needed\n",
        "  # when we use the autograd engine\n",
        "  # Predict\n",
        "  layer_1 = data.dot(weights_0_1)\n",
        "  layer_2 = layer_1.dot(weights_1_2)\n",
        "\n",
        "  # Compare\n",
        "  diff = (layer_2 - target)\n",
        "  sqdiff = (diff * diff)\n",
        "  # Mean Squared Error Loss\n",
        "  loss = sqdiff.sum(0)\n",
        "\n",
        "  # Learn this this is the backpropagation piece\n",
        "  layer_1_grad = diff.dot(weights_1_2.transpose())\n",
        "  weight_1_2_update = layer_1.transpose().dot(diff)\n",
        "  weight_0_1_update = data.transpose().dot(layer_1_grad)\n",
        "\n",
        "  # Update the weights\n",
        "  weights_1_2 -= weight_1_2_update * 0.1\n",
        "  weights_0_1 -= weight_0_1_update * 0.1\n",
        "  print(loss[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6imJfsG3mFQL",
        "outputId": "6ce061d5-7a42-4473-bb2f-9cc618ccb658"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.066439994622396\n",
            "0.4959907791902341\n",
            "0.4180671892167177\n",
            "0.35298133007809646\n",
            "0.2972549636567376\n",
            "0.24923260381633278\n",
            "0.20785392075862477\n",
            "0.17231260916265181\n",
            "0.14193744536652994\n",
            "0.11613979792168387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd Backprop Toy Example"
      ],
      "metadata": {
        "id": "qW--COpBoM6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "np.random.seed(0)\n",
        "\n",
        "# Why does the Tensor.data need to np.array() here?\n",
        "# I think its unneccessary... I already got the same loss result without having np.array here\n",
        "# My concern is that we are leveraging numpy in places in the autograd Tensor code.\n",
        "  # Numpy functions get used in sum, expand, mm...\n",
        "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd=True)\n",
        "target = Tensor(np.array([[0], [1], [0], [1]]), autograd=True)\n",
        "\n",
        "w = list()\n",
        "w.append(Tensor(np.random.rand(2,3), label='w1', autograd=True))\n",
        "w.append(Tensor(np.random.rand(3,1), label='w2', autograd=True))\n",
        "\n",
        "optim = SGD(parameters=w, alpha=0.1)\n",
        "for i in range(10):\n",
        "\n",
        "  # Predict\n",
        "  pred = data.mm(w[0]).mm(w[1])\n",
        "\n",
        "  # Compare\n",
        "  loss = ((pred-target)* (pred-target)).sum(0)\n",
        "\n",
        "  # Learn\n",
        "  loss.backward(Tensor(np.ones_like(loss.data)))\n",
        "  optim.step()\n",
        "  # print('w',w)\n",
        "  # Replace below code with call to SGD class 'optim.step()'\n",
        "  # for w_ in w:\n",
        "  #   # print(w_.data)\n",
        "  #   # print(w_.grad)\n",
        "  #   w_.data -= w_.grad.data * 0.1\n",
        "  #   w_.grad.data *= 0\n",
        "\n",
        "  print('loss',loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vWb7k5voJji",
        "outputId": "ebe5621b-8cec-4aa0-97f1-936fcdf3988e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss [0.58128304]\n",
            "loss [0.48988149]\n",
            "loss [0.41375111]\n",
            "loss [0.34489412]\n",
            "loss [0.28210124]\n",
            "loss [0.2254484]\n",
            "loss [0.17538853]\n",
            "loss [0.1324231]\n",
            "loss [0.09682769]\n",
            "loss [0.06849361]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "End Grokking Autograd Implementation"
      ],
      "metadata": {
        "id": "gGxQ8vxheh32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A toy example of RNN Backpropagation with exploding and vanishing gradients"
      ],
      "metadata": {
        "id": "Kepf5QuJfVvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "RcJTWeu_f285"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sigmoid,relu) = (lambda x: 1/1+np.exp(-x)), lambda x: (x>0).astype(float)*x\n",
        "weights = np.array([[1,4],[4,1]])\n",
        "activation = sigmoid(np.array([1, 0.01]))\n",
        "\n",
        "print(\"Sigmoid Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  activation = sigmoid(activation.dot(weights))\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\nSigmoid Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  # The derivative of sigmoid causes very small gradients when activation is very near 0 or 1\n",
        "  sigmoid_deriv = (activation) * (1-activation)\n",
        "   # Chain Rule\n",
        "  gradient = sigmoid_deriv * gradient\n",
        "  gradient = gradient.dot(weights.transpose()) # So this is also part of the chain rule???\n",
        "  print(gradient)\n",
        "\n",
        "print(\"\\nRelu Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  # The matrix multiplication causes exploding gradients that don't get squashed by a nonlinearity as in sigmoid\n",
        "  activation = relu(activation.dot(weights))\n",
        "\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\n Relu Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
        "  print(gradient)\n",
        "\n",
        "# Adding gates to RNN will replace all of the nonlinearies and matrix multiplications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ01X-TCfaVg",
        "outputId": "f54cbb9e-8696-423f-9af0-e8fb4bc85180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Activations\n",
            "[1.00008889 1.00057475]\n",
            "[1.00672188 1.00673168]\n",
            "[1.006515   1.00651519]\n",
            "[1.00652199 1.00652199]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "\n",
            "Sigmoid Gradients\n",
            "[-0.03282154 -0.03282154]\n",
            "[0.00107725 0.00107725]\n",
            "[-3.53571093e-05 -3.53571093e-05]\n",
            "[1.16047468e-06 1.16047468e-06]\n",
            "[-3.80885641e-08 -3.80885641e-08]\n",
            "[1.25012386e-09 1.25012385e-09]\n",
            "[-4.10323732e-11 -4.10323591e-11]\n",
            "[1.34536847e-12 1.34534485e-12]\n",
            "[-4.55737812e-14 -4.55341556e-14]\n",
            "[1.08795551e-16 4.23921767e-17]\n",
            "\n",
            "Relu Activations\n",
            "[5.00238791 5.00093033]\n",
            "[25.00610921 25.01048197]\n",
            "[125.04803709 125.03491883]\n",
            "[625.1877124  625.22706719]\n",
            "[3126.09598115 3125.97791678]\n",
            "[15630.00764826 15630.36184138]\n",
            "[78151.45501378 78150.39243441]\n",
            "[390753.02475143 390756.21248955]\n",
            "[1953777.87470964 1953768.31149529]\n",
            "[9768851.12069078 9768879.81033384]\n",
            "\n",
            " Relu Gradients\n",
            "[5. 5.]\n",
            "[25. 25.]\n",
            "[125. 125.]\n",
            "[625. 625.]\n",
            "[3125. 3125.]\n",
            "[15625. 15625.]\n",
            "[78125. 78125.]\n",
            "[390625. 390625.]\n",
            "[1953125. 1953125.]\n",
            "[9765625. 9765625.]\n"
          ]
        }
      ]
    }
  ]
}