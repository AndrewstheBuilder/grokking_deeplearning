{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1wIaHpQ6oXM6A6u4lcmV3d-Pdy7jyxykZ",
      "authorship_tag": "ABX9TyME7gNVjqCIZdwLD0EnuLfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewstheBuilder/grokking_deeplearning/blob/main/LSTM_CH14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning Framework"
      ],
      "metadata": {
        "id": "oos3EL_c9kre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSuxNcAiN7Ys",
        "outputId": "96226048-149b-41ef-964b-a3274b6dc469"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "q3Ydt8SW9hu3"
      },
      "outputs": [],
      "source": [
        "# Tensor Class. The Foundation of the Deep Learning Framework\n",
        "import numpy as np\n",
        "class Tensor (object):\n",
        "  def __init__(self,data,label,\n",
        "               autograd=False,\n",
        "               creators=None,\n",
        "               creation_op=None,\n",
        "               id=None):\n",
        "    self.label = label\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "    if(id is None):\n",
        "      id = np.random.randint(0,100000) # What is the likelyhood of producing the same id for a tensor in the same session?\n",
        "    self.id = id\n",
        "\n",
        "    if(creators is not None):\n",
        "      for c in creators:\n",
        "        # keeps track of how many children a tensor has\n",
        "        if(self.id not in c.children):\n",
        "          # Initialize c.children[self.id]\n",
        "          # We are giving the creator the children property\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          # Update counter for children\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    '''\n",
        "    Checks whether a tensor has received the correct\n",
        "    number of gradients from each child\n",
        "    '''\n",
        "    for id,cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  def backward(self,grad=None,grad_origin=None):\n",
        "    if(self.autograd):\n",
        "\n",
        "      if(grad is None):\n",
        "        grad = Tensor(np.ones_like(self.data), 'grad'+str(self.data.shape))\n",
        "\n",
        "      if(grad_origin is not None):\n",
        "        if(self.children[grad_origin.id] == 0):\n",
        "          print('self.label',self.label)\n",
        "          # print('self.children[grad_origin.id].label',self.children[grad_origin.id].label)\n",
        "          raise Exception(\"cannot backprop more than once\")\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      if(self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        # accumulates gradients from several children\n",
        "        self.grad += grad\n",
        "\n",
        "      if(self.creators is not None and\n",
        "         (self.all_children_grads_accounted_for() or\n",
        "          grad_origin is None)):\n",
        "          # begins actual back propagation\n",
        "          if(self.creation_op == \"add\"):\n",
        "            self.creators[0].backward(grad,self)\n",
        "            self.creators[1].backward(grad,self)\n",
        "\n",
        "          if(self.creation_op == \"neg\"):\n",
        "            self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "          if(self.creation_op == \"sub\"):\n",
        "            new = Tensor(self.grad.data, label='sub_grad')\n",
        "            self.creators[0].backward(new, self)\n",
        "            new = Tensor(self.grad.__neg__().data, label='sub_grad2')\n",
        "            self.creators[1].backward(new,self)\n",
        "\n",
        "          if(self.creation_op == \"mul\"):\n",
        "            new = self.grad * self.creators[1].data\n",
        "            self.creators[0].backward(new, self)\n",
        "            new = self.grad * self.creators[0].data\n",
        "            self.creators[1].backward(new, self)\n",
        "\n",
        "          if(self.creation_op == \"mm\"):\n",
        "            # Usually an activation\n",
        "            act = self.creators[0]\n",
        "            weights = self.creators[1]\n",
        "            new = self.grad.mm(weights.transpose())\n",
        "            act.backward(new)\n",
        "            new = self.grad.transpose().mm(act).transpose()\n",
        "            weights.backward(new)\n",
        "\n",
        "          if(self.creation_op == \"transpose\"):\n",
        "            self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "          if(\"sum\" in self.creation_op):\n",
        "            dim = int(self.creation_op.split(\"_\")[1])\n",
        "            ds = self.creators[0].data.shape[dim]\n",
        "            self.creators[0].backward(self.grad.expand(dim, ds))\n",
        "\n",
        "          if(\"expand\" in self.creation_op):\n",
        "            dim = int(self.creation_op.split(\"_\")[1])\n",
        "            self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "          if(self.creation_op == \"sigmoid\"):\n",
        "            ones = Tensor(np.ones_like(self.grad.data), \"ones used in sigmoid backprop\")\n",
        "            self.creators[0].backward(Tensor(self.grad.data * (self.data * (ones.data - self.data)), \"sigmoid_grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"tanh\"):\n",
        "            ones = Tensor(np.ones_like(self.grad.data))\n",
        "            self.creators[0].backward(Tensor(self.grad.data * (ones.data - self.data), \"tanh_grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"index_select\"):\n",
        "            new_grad = np.zeros_like(self.creators[0].data)\n",
        "            indices_ = self.index_select_indices.data.flatten()\n",
        "            grad_ = grad.data.reshape(len(indices_), -1)\n",
        "            for i in range(len(indices_)):\n",
        "              new_grad[indices_[i]] += grad_[i]\n",
        "            self.creators[0].backward(Tensor(new_grad, \"index_select grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"cross_entropy\"):\n",
        "            # This is the complicated derivation we did in part 4 of the makemore series\n",
        "            dx = self.softmax_output - self.target_dist\n",
        "            self.creators[0].backward(Tensor(dx, \"cross_entropy complicated deriv backprop dx\"))\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data,\n",
        "                    label=self.label+' + '+other.label,\n",
        "                    autograd = True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"add\")\n",
        "    return Tensor(self.data + other.data, 'add no grad')\n",
        "\n",
        "  def __neg__(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data*-1,\n",
        "                    label='-'+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"neg\",)\n",
        "    return Tensor(self.data*-1, 'neg no grad')\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data - other.data,\n",
        "                    label = self.label + ' - ' + other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"sub\")\n",
        "    return Tensor(self.data - other.data, 'sub no grad')\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data * other.data,\n",
        "                    label = self.label+'*'+other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"mul\")\n",
        "    return Tensor(self.data - other.data, 'mul no grad')\n",
        "\n",
        "  def sum(self, dim):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.sum(dim),\n",
        "                    label = self.label+'.sum_'+str(dim)+')',\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sum_\"+str(dim))\n",
        "    return Tensor(self.data.sum(dim), 'sum no grad')\n",
        "\n",
        "  def expand(self, dim, copies):\n",
        "\n",
        "    trans_cmd = list(range(0, len(self.data.shape)))\n",
        "    trans_cmd.insert(dim, len(self.data.shape))\n",
        "    new_shape = list(self.data.shape) + [copies]\n",
        "    new_data = self.data.repeat(copies).reshape(new_shape)\n",
        "    new_data = new_data.transpose(trans_cmd)\n",
        "\n",
        "    if(self.autograd):\n",
        "      return Tensor(new_data,\n",
        "                    label=self.label+\".expand_\"+str(dim),\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"expand_\"+str(dim))\n",
        "    return new_data\n",
        "\n",
        "  def transpose(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.transpose(),\n",
        "                    label=self.label+\".transpose\",\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"transpose\")\n",
        "    return Tensor(self.data.transpose(), \"transpose no grad\")\n",
        "\n",
        "  def mm(self,x):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.dot(x.data),\n",
        "                    label=self.label+\".dot_\"+x.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self,x],\n",
        "                    creation_op=\"mm\")\n",
        "    return Tensor(self.data.dot(x.data), \"mm no grad\")\n",
        "\n",
        "  def softmax(self):\n",
        "    temp = np.exp(self.data)\n",
        "    softmax_output = temp / np.sum(temp,\n",
        "                                   axis=len(self.data.shape)-1,\n",
        "                                   keepdims=True)\n",
        "    return softmax_output\n",
        "\n",
        "  # Nonlinearities\n",
        "  def sigmoid(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(1/(1+np.exp(-self.data)),\n",
        "                    label=\"sigmoid_\"+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sigmoid\")\n",
        "    return Tensor(1/(1+np.exp(-self.data)), label=\"(no auto grad)sigmoid_\"+self.label)\n",
        "\n",
        "  def tanh(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(np.tanh(self.data),\n",
        "                    label=\"tanh_\"+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"tanh\")\n",
        "    return Tensor(np.tanh(self.data), label=\"(no auto grad)tanh\"+self.label)\n",
        "\n",
        "  def index_select(self, indices):\n",
        "    if(self.autograd):\n",
        "      new = Tensor(self.data[indices.data],\n",
        "                   label=\"index_select w/ \"+self.label,\n",
        "                   autograd=True,\n",
        "                   creators=[self],\n",
        "                   creation_op=\"index_select\")\n",
        "      new.index_select_indices = indices\n",
        "      return new\n",
        "    return Tensor(self.data[indices.data], \"index_select no grad\")\n",
        "\n",
        "  def cross_entropy(self, target_indices):\n",
        "    temp = np.exp(self.data)\n",
        "    softmax_output = temp / np.sum(temp,\n",
        "                                   axis=len(self.data.shape)-1,\n",
        "                                   keepdims=True)\n",
        "    t = target_indices.data.flatten()\n",
        "    p = softmax_output.reshape(len(t),-1)\n",
        "    target_dist = np.eye(p.shape[1])[t]\n",
        "    loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "    if(self.autograd):\n",
        "      out = Tensor(loss,\n",
        "                   label=\"cross_entropy\",\n",
        "                   autograd=True,\n",
        "                   creators=[self],\n",
        "                   creation_op=\"cross_entropy\")\n",
        "      out.softmax_output = softmax_output\n",
        "      out.target_dist = target_dist\n",
        "      return out\n",
        "    return Tensor(loss, \"cross_entropy no grad\")\n",
        "\n",
        "  def __repr__(self):\n",
        "    # This method calls the self.data's repr method\n",
        "    return str(self.data.__repr__())\n",
        "    # return str(self.label.__repr__() + \":\" + self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())\n",
        "    # return str(self.label.__repr__() + \":\" + self.data.__str__() + ' Creators:'+self.creators.__str__())\n",
        "\n",
        "# Other Classes for the Deep Learning (DL) Framework\n",
        "class SGD(object):\n",
        "  def __init__(self, parameters, alpha=0.1):\n",
        "    self.parameters = parameters\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def zero(self):\n",
        "    # zero parameters' gradients\n",
        "    for p in self.parameters:\n",
        "      p.grad.data *= 0\n",
        "\n",
        "  def step(self, zero=True):\n",
        "    # update parameters' data based on their gradients\n",
        "    # zero out the gradient after if zero=True\n",
        "    for p in self.parameters:\n",
        "      p.data -= p.grad.data * self.alpha\n",
        "      if(zero):\n",
        "        p.grad.data *= 0\n",
        "\n",
        "# The Layer Class another foundation for the DL Framework\n",
        "class Layer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.parameters = list()\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "  def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "    super().__init__()\n",
        "    W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs))\n",
        "    self.weight = Tensor(W, autograd=True, label='Linear W')\n",
        "    self.bias = None\n",
        "    if(bias!=False):\n",
        "      self.bias = Tensor(np.zeros(n_outputs), autograd=True, label='Linear b')\n",
        "\n",
        "    self.parameters.append(self.weight)\n",
        "    self.parameters.append(self.bias)\n",
        "\n",
        "  def forward(self, input):\n",
        "    if(self.bias != None):\n",
        "      return input.mm(self.weight)+self.bias.expand(0, len(input.data))\n",
        "    return input.mm(self.weight)\n",
        "\n",
        "class Sequential(Layer):\n",
        "\n",
        "  def __init__(self, layers=list()):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def forward(self, input):\n",
        "    for layer in self.layers:\n",
        "      input = layer.forward(input)\n",
        "    return input\n",
        "\n",
        "  def get_parameters(self):\n",
        "    params = list()\n",
        "    for l in self.layers:\n",
        "      params += l.get_parameters()\n",
        "    return params\n",
        "\n",
        "class Tanh(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.tanh()\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.sigmoid()\n",
        "\n",
        "class Embedding(Layer):\n",
        "\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dim = dim\n",
        "\n",
        "    # this initialization style is a convention from word2vec\n",
        "    weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
        "    self.weight = Tensor(weight, label=\"Embedding weight\", autograd=True)\n",
        "\n",
        "    self.parameters.append(self.weight)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.weight.index_select(input)\n",
        "\n",
        "# You can also create layers that are functions on the input\n",
        "class MSELoss(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, pred, target):\n",
        "    return ((pred-target)*(pred-target)).sum(0)\n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    return input.cross_entropy(target)\n",
        "\n",
        "class RNNCell(Layer):\n",
        "  def __init__(self,n_inputs,n_hidden,n_output,activation='sigmoid'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_inputs = n_inputs\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_output = n_output\n",
        "\n",
        "    if(activation == 'sigmoid'):\n",
        "      self.activation = Sigmoid()\n",
        "    elif(activation == 'tanh'):\n",
        "      self.activation = Tanh()\n",
        "    else:\n",
        "      raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "    self.w_ih = Linear(n_inputs, n_hidden)\n",
        "    self.w_hh = Linear(n_hidden, n_hidden)\n",
        "    self.w_ho = Linear(n_hidden, n_output)\n",
        "\n",
        "    self.parameters += self.w_ih.get_parameters()\n",
        "    self.parameters += self.w_hh.get_parameters()\n",
        "    self.parameters += self.w_ho.get_parameters()\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "      from_prev_hidden = self.w_hh.forward(hidden)\n",
        "      combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "      new_hidden = self.activation.forward(combined)\n",
        "      output = self.w_ho.forward(new_hidden)\n",
        "      return output, new_hidden\n",
        "\n",
        "  def init_hidden(self, batch_size=1):\n",
        "    # What is this used for?\n",
        "    return Tensor(np.zeros((batch_size, self.n_hidden)), label=\"RNN Hidden State\", autograd=True)\n",
        "\n",
        "class LSTMCell(Layer):\n",
        "\n",
        "  def __init__(self, n_inputs, n_hidden, n_output):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_inputs = n_inputs\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_output = n_output\n",
        "\n",
        "    self.xf = Linear(n_inputs, n_hidden)\n",
        "    self.xi = Linear(n_inputs, n_hidden)\n",
        "    self.xo = Linear(n_inputs, n_hidden)\n",
        "    self.xc = Linear(n_inputs, n_hidden)\n",
        "    self.hf = Linear(n_inputs, n_hidden, bias=False)\n",
        "    self.hi = Linear(n_inputs, n_hidden, bias=False)\n",
        "    self.ho = Linear(n_inputs, n_hidden, bias=False)\n",
        "    self.hc = Linear(n_inputs, n_hidden, bias=False)\n",
        "\n",
        "    self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
        "\n",
        "    self.parameters += self.xf.get_parameters()\n",
        "    self.parameters += self.xi.get_parameters()\n",
        "    self.parameters += self.xo.get_parameters()\n",
        "    self.parameters += self.xc.get_parameters()\n",
        "    self.parameters += self.hf.get_parameters()\n",
        "    self.parameters += self.hi.get_parameters()\n",
        "    self.parameters += self.ho.get_parameters()\n",
        "    self.parameters += self.hc.get_parameters()\n",
        "\n",
        "    self.parameters += self.w_ho.get_parameters()\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "\n",
        "    prev_hidden = hidden[0]\n",
        "    prev_cell = hidden[1]\n",
        "\n",
        "    f=(self.xf.forward(input)+self.hf.forward(prev_hidden)).sigmoid()\n",
        "    i=(self.xi.forward(input)+self.hi.forward(prev_hidden)).sigmoid()\n",
        "    o=(self.xo.forward(input)+self.ho.forward(prev_hidden)).sigmoid()\n",
        "    g=(self.xc.forward(input)+self.hc.forward(prev_hidden)).tanh()\n",
        "    c = (f * prev_cell) + (i * g)\n",
        "    h = o * c.tanh()\n",
        "\n",
        "    output = self.w_ho.forward(h)\n",
        "    return output, (h,c)\n",
        "\n",
        "  def init_hidden(self, batch_size=1):\n",
        "    h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
        "    c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
        "    h.data[:, 0] += 1\n",
        "    c.data[:, 0] += 1\n",
        "    return (h,c)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "PlYNhqid_Eoe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sigmoid,relu) = (lambda x: 1/1+np.exp(-x)), lambda x: (x>0).astype(float)*x\n",
        "weights = np.array([[1,4],[4,1]])\n",
        "activation = sigmoid(np.array([1, 0.01]))\n",
        "\n",
        "print(\"Sigmoid Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  activation = sigmoid(activation.dot(weights))\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\nSigmoid Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  # The derivative of sigmoid causes very small gradients when activation is very near 0 or 1\n",
        "  sigmoid_deriv = (activation) * (1-activation)\n",
        "   # Chain Rule\n",
        "  gradient = sigmoid_deriv * gradient\n",
        "  gradient = gradient.dot(weights.transpose()) # So this is also part of the chain rule???\n",
        "  print(gradient)\n",
        "\n",
        "print(\"\\nRelu Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  # The matrix multiplication causes exploding gradients that don't get squashed by a nonlinearity as in sigmoid\n",
        "  activation = relu(activation.dot(weights))\n",
        "\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\n Relu Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
        "  print(gradient)\n",
        "\n",
        "# Adding gates to RNN will replace all of the nonlinearies and matrix multiplications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlBgOmQ0_BjK",
        "outputId": "8211212d-2dab-4a73-841a-bffcbd6a4ccf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Activations\n",
            "[1.00008889 1.00057475]\n",
            "[1.00672188 1.00673168]\n",
            "[1.006515   1.00651519]\n",
            "[1.00652199 1.00652199]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "\n",
            "Sigmoid Gradients\n",
            "[-0.03282154 -0.03282154]\n",
            "[0.00107725 0.00107725]\n",
            "[-3.53571093e-05 -3.53571093e-05]\n",
            "[1.16047468e-06 1.16047468e-06]\n",
            "[-3.80885641e-08 -3.80885641e-08]\n",
            "[1.25012386e-09 1.25012385e-09]\n",
            "[-4.10323732e-11 -4.10323591e-11]\n",
            "[1.34536847e-12 1.34534485e-12]\n",
            "[-4.55737812e-14 -4.55341556e-14]\n",
            "[1.08795551e-16 4.23921767e-17]\n",
            "\n",
            "Relu Activations\n",
            "[5.00238791 5.00093033]\n",
            "[25.00610921 25.01048197]\n",
            "[125.04803709 125.03491883]\n",
            "[625.1877124  625.22706719]\n",
            "[3126.09598115 3125.97791678]\n",
            "[15630.00764826 15630.36184138]\n",
            "[78151.45501378 78150.39243441]\n",
            "[390753.02475143 390756.21248955]\n",
            "[1953777.87470964 1953768.31149529]\n",
            "[9768851.12069078 9768879.81033384]\n",
            "\n",
            " Relu Gradients\n",
            "[5. 5.]\n",
            "[25. 25.]\n",
            "[125. 125.]\n",
            "[625. 625.]\n",
            "[3125. 3125.]\n",
            "[15625. 15625.]\n",
            "[78125. 78125.]\n",
            "[390625. 390625.]\n",
            "[1953125. 1953125.]\n",
            "[9765625. 9765625.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Implementation CH 14"
      ],
      "metadata": {
        "id": "iSScP21c_eyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, random, math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "pHyT7kJO_eOO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('drive/MyDrive/grokking/shakespear.txt','r')\n",
        "raw = f.read()\n",
        "f.close()\n",
        "\n",
        "vocab = list(set(raw))\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "  word2index[word] = i\n",
        "indices = np.array(list(map(lambda x:word2index[x], raw)))"
      ],
      "metadata": {
        "id": "76zlvR4iOQzc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('raw[:10]', raw[:10])\n",
        "print('indices.length',indices.shape)\n",
        "print(indices.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P32ibAORYd5",
        "outputId": "49fc797f-f624-4d4f-f09d-62099bab99e6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw[:10] That, poor\n",
            "indices.length (99993,)\n",
            "99993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batching\n",
        "batch_size = 32\n",
        "bptt = 16\n",
        "n_batches = int((indices.shape[0] / (batch_size)))\n",
        "trimmed_indices = indices[:n_batches*batch_size]\n",
        "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
        "# print('batched_indices[:5]',batched_indices[:5])\n",
        "# print('n_batches', n_batches)\n",
        "batched_indices = batched_indices.transpose() # Why do we transpose it?\n",
        "# print('batched_indices[:5] after transpose',batched_indices[:5])\n",
        "\n",
        "input_batched_indices = batched_indices[0:-1]\n",
        "target_batched_indices = batched_indices[1:]\n",
        "\n",
        "n_bptt = int(((n_batches-1) / bptt))\n",
        "input_batches = input_batched_indices[:n_bptt*bptt]\n",
        "input_batches = input_batches.reshape(n_bptt,bptt,batch_size)\n",
        "target_batches = target_batched_indices[:n_bptt*bptt]\n",
        "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
      ],
      "metadata": {
        "id": "RjkAooOQRtbN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_ll6uPqCeDSC",
        "outputId": "024cf25c-712c-4b97-deac-1b9f808feb36"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'That,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCZmnN_ieEjF",
        "outputId": "e699eaab-b99e-40a0-cc35-5594c1fb1b7c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33, 57, 19, 46, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2index['T']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enQJW8fdeGaU",
        "outputId": "2fcdc24e-740b-4800-c9a2-4399a5c4dcbc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batched_indices[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_sN7A9yebWf",
        "outputId": "00f57a6c-2c0c-40c3-c64a-06999efc18f7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[33, 60, 61, 15,  7, 57, 33, 11, 11, 19, 19, 18,  7, 11, 30, 13,\n",
              "        11, 55, 57, 61, 18, 19, 11, 11, 11,  1, 11,  1,  1, 11, 11,  0],\n",
              "       [57, 13, 13, 15, 40, 18, 19, 56, 55,  0, 37, 18, 37, 46, 11, 13,\n",
              "        52,  7,  7, 13, 37,  6, 52, 35, 57,  0, 29,  8,  0,  8, 47, 11],\n",
              "       [19, 13, 21, 18, 18, 19, 37, 28,  7, 11, 37, 61, 35, 57, 23, 44,\n",
              "        18, 56, 37, 13, 35, 18, 10, 18, 18, 27,  7, 30, 11,  7,  7, 52],\n",
              "       [46, 32, 33, 10, 11, 10, 52, 11, 56,  0, 11, 11, 60, 18, 18, 12,\n",
              "        18, 30, 55, 43, 28, 11,  7, 18, 10, 37, 56, 13,  5, 10,  8,  7],\n",
              "       [30, 12,  1, 21, 23, 11,  7, 10, 11,  7, 23, 49, 13, 11, 11, 22,\n",
              "         0, 11, 11,  7, 60, 55, 23, 40, 30, 19, 37, 12, 56, 18, 18, 55]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_batches[0][0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh05ZO8MedG3",
        "outputId": "a127d9ee-9a63-410f-c60b-b814d3f55637"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[33, 60, 61, 15,  7, 57, 33, 11, 11, 19, 19, 18,  7, 11, 30, 13,\n",
              "        11, 55, 57, 61, 18, 19, 11, 11, 11,  1, 11,  1,  1, 11, 11,  0],\n",
              "       [57, 13, 13, 15, 40, 18, 19, 56, 55,  0, 37, 18, 37, 46, 11, 13,\n",
              "        52,  7,  7, 13, 37,  6, 52, 35, 57,  0, 29,  8,  0,  8, 47, 11],\n",
              "       [19, 13, 21, 18, 18, 19, 37, 28,  7, 11, 37, 61, 35, 57, 23, 44,\n",
              "        18, 56, 37, 13, 35, 18, 10, 18, 18, 27,  7, 30, 11,  7,  7, 52],\n",
              "       [46, 32, 33, 10, 11, 10, 52, 11, 56,  0, 11, 11, 60, 18, 18, 12,\n",
              "        18, 30, 55, 43, 28, 11,  7, 18, 10, 37, 56, 13,  5, 10,  8,  7],\n",
              "       [30, 12,  1, 21, 23, 11,  7, 10, 11,  7, 23, 49, 13, 11, 11, 22,\n",
              "         0, 11, 11,  7, 60, 55, 23, 40, 30, 19, 37, 12, 56, 18, 18, 55]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_batches[0][0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkfmP5k9egqO",
        "outputId": "981719fb-33cf-4a40-a8f3-d815aa9425c0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[57, 13, 13, 15, 40, 18, 19, 56, 55,  0, 37, 18, 37, 46, 11, 13,\n",
              "        52,  7,  7, 13, 37,  6, 52, 35, 57,  0, 29,  8,  0,  8, 47, 11],\n",
              "       [19, 13, 21, 18, 18, 19, 37, 28,  7, 11, 37, 61, 35, 57, 23, 44,\n",
              "        18, 56, 37, 13, 35, 18, 10, 18, 18, 27,  7, 30, 11,  7,  7, 52],\n",
              "       [46, 32, 33, 10, 11, 10, 52, 11, 56,  0, 11, 11, 60, 18, 18, 12,\n",
              "        18, 30, 55, 43, 28, 11,  7, 18, 10, 37, 56, 13,  5, 10,  8,  7],\n",
              "       [30, 12,  1, 21, 23, 11,  7, 10, 11,  7, 23, 49, 13, 11, 11, 22,\n",
              "         0, 11, 11,  7, 60, 55, 23, 40, 30, 19, 37, 12, 56, 18, 18, 55],\n",
              "       [11, 34, 28, 35,  1, 18, 46, 18, 23, 11, 18, 57, 13, 35, 23, 24,\n",
              "        11,  8, 52, 28, 13,  7, 11, 11, 11, 23, 35,  0, 15, 11, 30, 28]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
        "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
        "criterion = CrossEntropyLoss()\n",
        "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
      ],
      "metadata": {
        "id": "6dLc-8RTXeKq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(n=30, init_char=' '):\n",
        "  s = \"\"\n",
        "  hidden = model.init_hidden(batch_size=1)\n",
        "  input = Tensor(np.array([word2index[init_char]]), 'generate_sample init input')\n",
        "  for i in range(n):\n",
        "    rnn_input = embed.forward(input)\n",
        "    output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "    output.data *= 10 # Temperature for sampling. Higher = greedier\n",
        "    temp_dist = output.softmax()\n",
        "    temp_dist /= temp_dist.sum()\n",
        "\n",
        "    m = (temp_dist > np.random.randn()).argmax() # Samples from pred\n",
        "    c = vocab[m]\n",
        "    input = Tensor(np.array([m]), 'generate_sample input')\n",
        "    s += c\n",
        "  return s"
      ],
      "metadata": {
        "id": "W3BPtJaYaOWq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_sample(n=2000, init_char='\\n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-1TFW0xbOAY",
        "outputId": "dceb0db4-4cb2-4274-d67e-1ad776253115"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mnnw? nnnnnnnnnnnnnonnownnn nnnnownnounnnnnnononnnnonnnAncnnn nonknn nonknnnnownnnnnnn nnmannnnnonnnnongnonnn nonkn,n nnnnownnnnouns:nn notnnnnnnnonourn annn nnmnn boy,nnn nonnnnnnonnnnnnow,nnnnnnnnnnnnnonnnnonnnnnnnnnnounsnonndnynnn nownnononnnnononour wnnnnnnn wilnnn nongmonn notnnnnnnnAnnownnn nnnnnnnnAnnn winn nnnnonnonnnA bntnnnnn mnngnnn notn\n",
            "AnnnnnA nnnnonnonnonnnAndnntn'nnnnownnn wnncnnnnnnnnAnnnnown;nI, snnonnnnn nnnnonounnnAnd.nn,n nnn nnnnn nnn nnn mnngninnnononn:\n",
            "nnnnonnnnonnonnnAnnn nonknn and nntnongnougn;ndnnnnnnnnnonnnnonn nomnnnnnnnnonnonnnnnA bnnn mantnnnn,ninnnounnnnnonnnAncennn nonnonnnnononnnnonn nonnnnnnnnnnnnAnt.nnonnonnnnownnnnnnonnowsninnnnnnn nnnnonnon nnannn nnmny;nnnn nnnnnnnonnonnnnonould nn nonknn nnn nnmy nnononounsnnn nnn nonkn,nnn wnnnnn won mnnow, nnnnn nonk nnnnnn notnninnnounnnnnnnnnA nn nnn nnnnnnnnAnnn nonknongnnsnnnnnnnnonnonnnA bnnn nnn nnnnow,nnnnonn nnnnnn nonn:\n",
            "Inn nnnnonnonn notnnn nnmnnn nnn nnnnnnonnnnnnnnnnoulnnAlnnnnAnnnnnnnnould\n",
            "nonn:\n",
            "nn ent nn nn nnn wnnIncelnn nn sonfnotnnn nnmnnownninnnounsnownt nnthnnnnnnnnnnnnnouln\n",
            "Ann nntnisn thnnnnnA nnn wnnnnn wnnnnn nnn nnnnnnow, Cinknnn nnI sloononnnonononnnAnnonnnA nnn nonnnA bnnnnowntnnn wnnnnnnonounsnonnnnnnnow,nnn,nnnnonnnnnnonnnnnnnnnnnow,niarnnn,nnnnnononn:n;nnnn wnnnnnnononnnnnnononnnnmandn;\n",
            "nnnnnnnnnnnonnonnnnnAndnnnnownnn mnynnnnonnnnnAnnn wnit,ningnongm;\n",
            "Tnnn nnn nonnnA bntucnnAndn,nnnnonounnnnnownnnnnnnnonnnAnnonnnnnnnnnnnnnonnonnnnnA nonnnnown nna nnnnonnnnnnnnnow, nn nnmann nntnnnn nnn nnnnnnnnnnnnnnnnononnnnnnnA nn nn bent nnnnnnnn mnnnnnnnnonnown nnancnounnnnA nn snnotnessnnnn mannnnonnnnnnnAnnonounn,nnnnnnnownnounnnnAnnonnnAnnownnn nnnonnnAnnnnnnnowsnnnnononnnonnnnnA bnnnnnnnnnnAncnouse, nnn winnnnows nannownnn wnnnnnnonnon nna bnnnnononnnnn nnmnynnn mnnnnnnn nnnnownnn nnnnn nonnnnonnnnown nancnnnnnn nnn nnn nnnnnnnnnnnononnnnmand nnnnn nnnnnnnnnown nnnnnnnonononnnnownnnnonnonnnA nnnnnonnnnnnnAnnmyningnougnnnnn nom.n'd nn nnnnn nonnnnnnnAnnownnn winn ennnnnnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` python\n",
        "# This causes a backprop too many times error\n",
        "def train(iterations=100):\n",
        "  for iter in range(iterations):\n",
        "    total_loss = 0\n",
        "    n_loss = 0\n",
        "\n",
        "    hidden = model.init_hidden(batch_size=batch_size)\n",
        "    for batch_i in range(len(input_batches)):\n",
        "      hidden = Tensor(hidden.data, 'hidden', autograd=True)\n",
        "      # loss = None\n",
        "      losses = list()\n",
        "      for t in range(bptt):\n",
        "        input = Tensor(input_batches[batch_i][t], 'input', autograd=True)\n",
        "        rnn_input = embed.forward(input=input)\n",
        "        output, hidden = model.forward(input=rnn_input,\n",
        "                                       hidden=hidden)\n",
        "        target = Tensor(target_batches[batch_i][t], 'target', autograd=True)\n",
        "        batch_loss = criterion.forward(output, target)\n",
        "        losses.append(batch_loss)\n",
        "        if(t == 0):\n",
        "          loss = batch_loss\n",
        "        else:\n",
        "          loss = loss + batch_loss\n",
        "      for loss in losses:\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss.data\n",
        "      log = \"\\r Iter:\" + str(iter)\n",
        "      log += \" - Batch \" + str(batch_i+1)+\"/\"+str(len(input_batches))\n",
        "      log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
        "      if(batch_i == 0):\n",
        "        log += \" - \" + generate_sample(70, '\\n').replace(\"\\n\", \" \")\n",
        "      if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
        "        sys.stdout.write(log)\n",
        "    optim.alpha *= 0.99 # Decaying the learning rate\n",
        "    print()\n",
        "train()\n",
        "```"
      ],
      "metadata": {
        "id": "1zZXvzKvDZ85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(iterations=400):\n",
        "    for iter in range(iterations):\n",
        "        total_loss = 0\n",
        "        n_loss = 0\n",
        "\n",
        "        hidden = model.init_hidden(batch_size=batch_size)\n",
        "        for batch_i in range(len(input_batches)):\n",
        "\n",
        "            hidden = Tensor(hidden.data, 'hidden', autograd=True)\n",
        "            loss = None\n",
        "            losses = list()\n",
        "            for t in range(bptt):\n",
        "                input = Tensor(input_batches[batch_i][t], 'input', autograd=True)\n",
        "                rnn_input = embed.forward(input=input)\n",
        "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "\n",
        "                target = Tensor(target_batches[batch_i][t], 'target', autograd=True)\n",
        "                batch_loss = criterion.forward(output, target)\n",
        "                losses.append(batch_loss)\n",
        "                if(t == 0):\n",
        "                    loss = batch_loss\n",
        "                else:\n",
        "                    loss = loss + batch_loss\n",
        "            for loss in losses:\n",
        "                \"\"\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            total_loss += loss.data\n",
        "            log = \"\\r Iter:\" + str(iter)\n",
        "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
        "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
        "            if(batch_i == 0):\n",
        "                log += \" - \" + generate_sample(n=70, init_char='\\n').replace(\"\\n\",\" \")\n",
        "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
        "                sys.stdout.write(log)\n",
        "        optim.alpha *= 0.99\n",
        "        print()"
      ],
      "metadata": {
        "id": "t_tdz69gCUjX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "voj-TQuRaNcU",
        "outputId": "36721363-3d51-4acd-8bdb-35239940f9e1"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Iter:0 - Batch 191/195 - Loss:1.563947466137868\n",
            " Iter:1 - Batch 191/195 - Loss:1.4945942745932106\n",
            " Iter:2 - Batch 191/195 - Loss:1.4412511250651394\n",
            " Iter:3 - Batch 191/195 - Loss:1.399385869515005\n",
            " Iter:4 - Batch 191/195 - Loss:1.368439503871153\n",
            " Iter:5 - Batch 191/195 - Loss:1.333318596910202\n",
            " Iter:6 - Batch 41/195 - Loss:1.3252919504176002"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-51afd2885487>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-351de883dc51>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterations)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cd0ce936c885>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m    408\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m       \u001b[0mfrom_prev_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_hh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m       \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_ih\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfrom_prev_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m       \u001b[0mnew_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_ho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cd0ce936c885>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cd0ce936c885>\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       return Tensor(self.data + other.data,\n\u001b[0m\u001b[1;32m    129\u001b[0m                     \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' + '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mautograd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-cd0ce936c885>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, autograd, creators, creation_op, id)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   def __init__(self,data,label,\n\u001b[0m\u001b[1;32m      5\u001b[0m                \u001b[0mautograd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0mcreators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}