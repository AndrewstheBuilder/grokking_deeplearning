{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wIaHpQ6oXM6A6u4lcmV3d-Pdy7jyxykZ",
      "authorship_tag": "ABX9TyNCc7AupbUtoZMfg+JzU/cx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewstheBuilder/grokking_deeplearning/blob/main/LSTM_CH14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning Framework"
      ],
      "metadata": {
        "id": "oos3EL_c9kre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSuxNcAiN7Ys",
        "outputId": "1b6638f4-c21f-4eff-9c9c-543553fe8b63"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "q3Ydt8SW9hu3"
      },
      "outputs": [],
      "source": [
        "# Tensor Class. The Foundation of the Deep Learning Framework\n",
        "import numpy as np\n",
        "class Tensor (object):\n",
        "  def __init__(self,data,label='None',\n",
        "               autograd=False,\n",
        "               creators=None,\n",
        "               creation_op=None,\n",
        "               id=None):\n",
        "    self.label = label\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "    if(id is None):\n",
        "      id = np.random.randint(0,100000) # What is the likelyhood of producing the same id for a tensor in the same session?\n",
        "    self.id = id\n",
        "\n",
        "    if(creators is not None):\n",
        "      for c in creators:\n",
        "        # keeps track of how many children a tensor has\n",
        "        if(self.id not in c.children):\n",
        "          # Initialize c.children[self.id]\n",
        "          # We are giving the creator the children property\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          # Update counter for children\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    '''\n",
        "    Checks whether a tensor has received the correct\n",
        "    number of gradients from each child\n",
        "    '''\n",
        "    for id,cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  def backward(self,grad=None,grad_origin=None):\n",
        "    if(self.autograd):\n",
        "\n",
        "      if(grad is None):\n",
        "        grad = Tensor(np.ones_like(self.data), 'grad'+str(self.data.shape))\n",
        "\n",
        "      if(grad_origin is not None):\n",
        "        if(self.children[grad_origin.id] == 0):\n",
        "          raise Exception(\"cannot backprop more than once\")\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      if(self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        # accumulates gradients from several children\n",
        "        self.grad += grad\n",
        "\n",
        "      if(self.creators is not None and\n",
        "         (self.all_children_grads_accounted_for() or\n",
        "          grad_origin is None)):\n",
        "          # begins actual back propagation\n",
        "          if(self.creation_op == \"add\"):\n",
        "            self.creators[0].backward(grad,self)\n",
        "            self.creators[1].backward(grad,self)\n",
        "\n",
        "          if(self.creation_op == \"neg\"):\n",
        "            self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "          if(self.creation_op == \"sub\"):\n",
        "            new = Tensor(self.grad.data, label='sub_grad')\n",
        "            self.creators[0].backward(new, self)\n",
        "            new = Tensor(self.grad.__neg__().data, label='sub_grad2')\n",
        "            self.creators[1].backward(new,self)\n",
        "\n",
        "          if(self.creation_op == \"mul\"):\n",
        "            new = self.grad * self.creators[1].data\n",
        "            self.creators[0].backward(new, self)\n",
        "            new = self.grad * self.creators[0].data\n",
        "            self.creators[1].backward(new, self)\n",
        "\n",
        "          if(self.creation_op == \"mm\"):\n",
        "            # Usually an activation\n",
        "            act = self.creators[0]\n",
        "            weights = self.creators[1]\n",
        "            new = self.grad.mm(weights.transpose())\n",
        "            act.backward(new)\n",
        "            new = self.grad.transpose().mm(act).transpose()\n",
        "            weights.backward(new)\n",
        "\n",
        "          if(self.creation_op == \"transpose\"):\n",
        "            self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "          if(\"sum\" in self.creation_op):\n",
        "            dim = int(self.creation_op.split(\"_\")[1])\n",
        "            ds = self.creators[0].data.shape[dim]\n",
        "            self.creators[0].backward(self.grad.expand(dim, ds))\n",
        "\n",
        "          if(\"expand\" in self.creation_op):\n",
        "            dim = int(self.creation_op.split(\"_\")[1])\n",
        "            self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "          if(self.creation_op == \"sigmoid\"):\n",
        "            ones = Tensor(np.ones_like(self.grad.data), \"ones used in sigmoid backprop\")\n",
        "            self.creators[0].backward(Tensor(self.grad.data * (self.data * (ones.data - self.data)), \"sigmoid_grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"tanh\"):\n",
        "            ones = Tensor(np.ones_like(self.grad.data))\n",
        "            self.creators[0].backward(Tensor(self.grad.data * (ones.data - self.data), \"tanh_grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"index_select\"):\n",
        "            new_grad = np.zeros_like(self.creators[0].data)\n",
        "            indices_ = self.index_select_indices.data.flatten()\n",
        "            grad_ = grad.data.reshape(len(indices_), -1)\n",
        "            for i in range(len(indices_)):\n",
        "              new_grad[indices_[i]] += grad_[i]\n",
        "            self.creators[0].backward(Tensor(new_grad, \"index_select grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"cross_entropy\"):\n",
        "            # This is the complicated derivation we did in part 4 of the makemore series\n",
        "            dx = self.softmax_output - self.target_dist\n",
        "            self.creators[0].backward(Tensor(dx, \"cross_entropy complicated deriv backprop dx\"))\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data,\n",
        "                    label=self.label+' + '+other.label,\n",
        "                    autograd = True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"add\")\n",
        "    return Tensor(self.data + other.data, 'add no grad')\n",
        "\n",
        "  def __neg__(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data*-1,\n",
        "                    label='-'+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"neg\",)\n",
        "    return Tensor(self.data*-1, 'neg no grad')\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data - other.data,\n",
        "                    label = self.label + ' - ' + other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"sub\")\n",
        "    return Tensor(self.data - other.data, 'sub no grad')\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data * other.data,\n",
        "                    label = self.label+'*'+other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"mul\")\n",
        "    return Tensor(self.data - other.data, 'mul no grad')\n",
        "\n",
        "  def sum(self, dim):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.sum(dim),\n",
        "                    label = self.label+'.sum_'+str(dim)+')',\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sum_\"+str(dim))\n",
        "    return Tensor(self.data.sum(dim), 'sum no grad')\n",
        "\n",
        "  def expand(self, dim, copies):\n",
        "\n",
        "    trans_cmd = list(range(0, len(self.data.shape)))\n",
        "    trans_cmd.insert(dim, len(self.data.shape))\n",
        "    new_shape = list(self.data.shape) + [copies]\n",
        "    new_data = self.data.repeat(copies).reshape(new_shape)\n",
        "    new_data = new_data.transpose(trans_cmd)\n",
        "\n",
        "    if(self.autograd):\n",
        "      return Tensor(new_data,\n",
        "                    label=self.label+\".expand_\"+str(dim),\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"expand_\"+str(dim))\n",
        "    return new_data\n",
        "\n",
        "  def transpose(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.transpose(),\n",
        "                    label=self.label+\".transpose\",\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"transpose\")\n",
        "    return Tensor(self.data.transpose(), \"transpose no grad\")\n",
        "\n",
        "  def mm(self,x):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.dot(x.data),\n",
        "                    label=self.label+\".dot_\"+x.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self,x],\n",
        "                    creation_op=\"mm\")\n",
        "    return Tensor(self.data.dot(x.data), \"mm no grad\")\n",
        "\n",
        "  def softmax(self):\n",
        "    temp = np.exp(self.data)\n",
        "    softmax_output = temp / np.sum(temp,\n",
        "                                   axis=len(self.data.shape)-1,\n",
        "                                   keepdims=True)\n",
        "    return softmax_output\n",
        "\n",
        "  # Nonlinearities\n",
        "  def sigmoid(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(1/(1+np.exp(-self.data)),\n",
        "                    label=\"sigmoid_\"+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sigmoid\")\n",
        "    return Tensor(1/(1+np.exp(-self.data)), label=\"(no auto grad)sigmoid_\"+self.label)\n",
        "\n",
        "  def tanh(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(np.tanh(self.data),\n",
        "                    label=\"tanh_\"+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"tanh\")\n",
        "    return Tensor(np.tanh(self.data), label=\"(no auto grad)tanh\"+self.label)\n",
        "\n",
        "  def index_select(self, indices):\n",
        "    if(self.autograd):\n",
        "      new = Tensor(self.data[indices.data],\n",
        "                   label=\"index_select w/ \"+self.label,\n",
        "                   autograd=True,\n",
        "                   creators=[self],\n",
        "                   creation_op=\"index_select\")\n",
        "      new.index_select_indices = indices\n",
        "      return new\n",
        "    return Tensor(self.data[indices.data], \"index_select no grad\")\n",
        "\n",
        "  def cross_entropy(self, target_indices):\n",
        "    temp = np.exp(self.data)\n",
        "    softmax_output = temp / np.sum(temp,\n",
        "                                   axis=len(self.data.shape)-1,\n",
        "                                   keepdims=True)\n",
        "    t = target_indices.data.flatten()\n",
        "    p = softmax_output.reshape(len(t),-1)\n",
        "    target_dist = np.eye(p.shape[1])[t]\n",
        "    loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "    if(self.autograd):\n",
        "      out = Tensor(loss,\n",
        "                   label=\"cross_entropy\",\n",
        "                   autograd=True,\n",
        "                   creators=[self],\n",
        "                   creation_op=\"cross_entropy\")\n",
        "      out.softmax_output = softmax_output\n",
        "      out.target_dist = target_dist\n",
        "      return out\n",
        "    return Tensor(loss, \"cross_entropy no grad\")\n",
        "\n",
        "  def __repr__(self):\n",
        "    # This method calls the self.data's repr method\n",
        "    return str(self.data.__repr__())\n",
        "    # return str(self.label.__repr__() + \":\" + self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())\n",
        "    # return str(self.label.__repr__() + \":\" + self.data.__str__() + ' Creators:'+self.creators.__str__())\n",
        "\n",
        "# Other Classes for the Deep Learning (DL) Framework\n",
        "class SGD(object):\n",
        "  def __init__(self, parameters, alpha=0.1):\n",
        "    self.parameters = parameters\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def zero(self):\n",
        "    # zero parameters' gradients\n",
        "    for p in self.parameters:\n",
        "      p.grad.data *= 0\n",
        "\n",
        "  def step(self, zero=True):\n",
        "    # update parameters' data based on their gradients\n",
        "    # zero out the gradient after if zero=True\n",
        "    for p in self.parameters:\n",
        "      p.data -= p.grad.data * self.alpha\n",
        "      if(zero):\n",
        "        p.grad.data *= 0\n",
        "\n",
        "# The Layer Class another foundation for the DL Framework\n",
        "class Layer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.parameters = list()\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    super().__init__()\n",
        "    W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs))\n",
        "    self.weight = Tensor(W, autograd=True, label='Linear W')\n",
        "    self.bias = Tensor(np.zeros(n_outputs), autograd=True, label='Linear b')\n",
        "\n",
        "    self.parameters.append(self.weight)\n",
        "    self.parameters.append(self.bias)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.mm(self.weight)+self.bias.expand(0, len(input.data))\n",
        "\n",
        "class Sequential(Layer):\n",
        "\n",
        "  def __init__(self, layers=list()):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def forward(self, input):\n",
        "    for layer in self.layers:\n",
        "      input = layer.forward(input)\n",
        "    return input\n",
        "\n",
        "  def get_parameters(self):\n",
        "    params = list()\n",
        "    for l in self.layers:\n",
        "      params += l.get_parameters()\n",
        "    return params\n",
        "\n",
        "class Tanh(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.tanh()\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.sigmoid()\n",
        "\n",
        "class Embedding(Layer):\n",
        "\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dim = dim\n",
        "\n",
        "    # this initialization style is a convention from word2vec\n",
        "    weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
        "    self.weight = Tensor(weight, label=\"Embedding weight\", autograd=True)\n",
        "\n",
        "    self.parameters.append(self.weight)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.weight.index_select(input)\n",
        "\n",
        "# You can also create layers that are functions on the input\n",
        "class MSELoss(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, pred, target):\n",
        "    return ((pred-target)*(pred-target)).sum(0)\n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    return input.cross_entropy(target)\n",
        "\n",
        "class RNNCell(Layer):\n",
        "  def __init__(self,n_inputs,n_hidden,n_output,activation='sigmoid'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_inputs = n_inputs\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_output = n_output\n",
        "\n",
        "    if(activation == 'sigmoid'):\n",
        "      self.activation = Sigmoid()\n",
        "    elif(activation == 'tanh'):\n",
        "      self.activation = Tanh()\n",
        "    else:\n",
        "      raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "    self.w_ih = Linear(n_inputs, n_hidden)\n",
        "    self.w_hh = Linear(n_hidden, n_hidden)\n",
        "    self.w_ho = Linear(n_hidden, n_output)\n",
        "\n",
        "    self.parameters += self.w_ih.get_parameters()\n",
        "    self.parameters += self.w_hh.get_parameters()\n",
        "    self.parameters += self.w_ho.get_parameters()\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "      from_prev_hidden = self.w_hh.forward(hidden)\n",
        "      combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "      new_hidden = self.activation.forward(combined)\n",
        "      output = self.w_ho.forward(new_hidden)\n",
        "      return output, new_hidden\n",
        "\n",
        "  def init_hidden(self, batch_size=1):\n",
        "    # What is this used for?\n",
        "    return Tensor(np.zeros((batch_size, self.n_hidden)), label=\"RNN Hidden State\", autograd=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "PlYNhqid_Eoe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sigmoid,relu) = (lambda x: 1/1+np.exp(-x)), lambda x: (x>0).astype(float)*x\n",
        "weights = np.array([[1,4],[4,1]])\n",
        "activation = sigmoid(np.array([1, 0.01]))\n",
        "\n",
        "print(\"Sigmoid Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  activation = sigmoid(activation.dot(weights))\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\nSigmoid Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  # The derivative of sigmoid causes very small gradients when activation is very near 0 or 1\n",
        "  sigmoid_deriv = (activation) * (1-activation)\n",
        "   # Chain Rule\n",
        "  gradient = sigmoid_deriv * gradient\n",
        "  gradient = gradient.dot(weights.transpose()) # So this is also part of the chain rule???\n",
        "  print(gradient)\n",
        "\n",
        "print(\"\\nRelu Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  # The matrix multiplication causes exploding gradients that don't get squashed by a nonlinearity as in sigmoid\n",
        "  activation = relu(activation.dot(weights))\n",
        "\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\n Relu Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
        "  print(gradient)\n",
        "\n",
        "# Adding gates to RNN will replace all of the nonlinearies and matrix multiplications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlBgOmQ0_BjK",
        "outputId": "88de935d-9b2a-47fa-b78d-e86956a89fc3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Activations\n",
            "[1.00008889 1.00057475]\n",
            "[1.00672188 1.00673168]\n",
            "[1.006515   1.00651519]\n",
            "[1.00652199 1.00652199]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "\n",
            "Sigmoid Gradients\n",
            "[-0.03282154 -0.03282154]\n",
            "[0.00107725 0.00107725]\n",
            "[-3.53571093e-05 -3.53571093e-05]\n",
            "[1.16047468e-06 1.16047468e-06]\n",
            "[-3.80885641e-08 -3.80885641e-08]\n",
            "[1.25012386e-09 1.25012385e-09]\n",
            "[-4.10323732e-11 -4.10323591e-11]\n",
            "[1.34536847e-12 1.34534485e-12]\n",
            "[-4.55737812e-14 -4.55341556e-14]\n",
            "[1.08795551e-16 4.23921767e-17]\n",
            "\n",
            "Relu Activations\n",
            "[5.00238791 5.00093033]\n",
            "[25.00610921 25.01048197]\n",
            "[125.04803709 125.03491883]\n",
            "[625.1877124  625.22706719]\n",
            "[3126.09598115 3125.97791678]\n",
            "[15630.00764826 15630.36184138]\n",
            "[78151.45501378 78150.39243441]\n",
            "[390753.02475143 390756.21248955]\n",
            "[1953777.87470964 1953768.31149529]\n",
            "[9768851.12069078 9768879.81033384]\n",
            "\n",
            " Relu Gradients\n",
            "[5. 5.]\n",
            "[25. 25.]\n",
            "[125. 125.]\n",
            "[625. 625.]\n",
            "[3125. 3125.]\n",
            "[15625. 15625.]\n",
            "[78125. 78125.]\n",
            "[390625. 390625.]\n",
            "[1953125. 1953125.]\n",
            "[9765625. 9765625.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Implementation CH 14"
      ],
      "metadata": {
        "id": "iSScP21c_eyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, random, math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "pHyT7kJO_eOO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('drive/MyDrive/grokking/shakespear.txt','r')\n",
        "raw = f.read()\n",
        "f.close()\n",
        "\n",
        "vocab = list(set(raw))\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "  word2index[word] = i\n",
        "indices = np.array(list(map(lambda x:word2index[x], raw)))"
      ],
      "metadata": {
        "id": "76zlvR4iOQzc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('raw.length', len(raw))\n",
        "print('indices.length',indices.shape)\n",
        "print(indices.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P32ibAORYd5",
        "outputId": "d668ca1b-6584-49a7-edb0-ef01e09024df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw.length 94346\n",
            "indices.length (94346,)\n",
            "94346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batching\n",
        "batch_size = 32\n",
        "bptt = 16\n",
        "n_batches = int((indices.shape[0] / (batch_size)))\n",
        "trimmed_indices = indices[:n_batches*batch_size]\n",
        "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
        "# print('batched_indices[:5]',batched_indices[:5])\n",
        "# print('n_batches', n_batches)\n",
        "batched_indices = batched_indices.transpose() # Why do we transpose it?\n",
        "# print('batched_indices[:5] after transpose',batched_indices[:5])\n",
        "\n",
        "input_batched_indices = batched_indices[0:-1]\n",
        "target_batched_indices = batched_indices[1:]\n",
        "\n",
        "n_bptt = int(((n_batches-1) / bptt))\n",
        "input_batches = input_batched_indices[:n_bptt*bptt]\n",
        "input_batches = input_batches.reshape(n_bptt,bptt,batch_size)\n",
        "target_batches = target_batched_indices[:n_bptt*bptt]\n",
        "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)"
      ],
      "metadata": {
        "id": "RjkAooOQRtbN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
        "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
        "criterion = CrossEntropyLoss()\n",
        "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
      ],
      "metadata": {
        "id": "6dLc-8RTXeKq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(n=30, init_char=' '):\n",
        "  s = \"\"\n",
        "  hidden = model.init_hidden(batch_size=1)\n",
        "  input = Tensor(np.array([word2index[init_char]]))\n",
        "  for i in range(n):\n",
        "    rnn_input = embed.forward(input)\n",
        "    output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "    output.data *= 10 # Temperature for sampling. Higher = greedier\n",
        "    temp_dist = output.softmax()\n",
        "    temp_dist /= temp_dist.sum()\n",
        "\n",
        "    m = (temp_dist > np.random.randn()).argmax() # Samples from pred\n",
        "    c = vocab[m]\n",
        "    input = Tensor(np.array([m]))\n",
        "    s += c\n",
        "  return s"
      ],
      "metadata": {
        "id": "W3BPtJaYaOWq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_sample(n=2000, init_char='\\n'))"
      ],
      "metadata": {
        "id": "7-1TFW0xbOAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(iterations=100):\n",
        "  for iter in range(iterations):\n",
        "    total_loss = 0\n",
        "    n_loss = 0\n",
        "\n",
        "    hidden = model.init_hidden(batch_size=batch_size)\n",
        "    for batch_i in range(len(input_batches)):\n",
        "      hidden = Tensor(hidden.data, autograd=True)\n",
        "      loss = None\n",
        "      losses = list()\n",
        "      for t in range(bptt):\n",
        "        input = Tensor(input_batches[batch_i][t], autograd=True)\n",
        "        rnn_input = embed.forward(input=input)\n",
        "        output, hidden = model.forward(input=rnn_input,\n",
        "                                       hidden=hidden)\n",
        "        target = Tensor(target_batches[batch_i][t], autograd=True)\n",
        "        batch_loss = criterion.forward(output, target)\n",
        "        losses.append(batch_loss)\n",
        "        # if(t == 0):\n",
        "        #   loss = batch_loss\n",
        "        # else:\n",
        "        #   loss = loss + batch_loss\n",
        "      for loss in losses:\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss.data\n",
        "      log = \"\\r Iter:\" + str(iter)\n",
        "      log += \" - Batch \" + str(batch_i+1)+\"/\"+str(len(input_batches))\n",
        "      log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
        "      if(batch_i == 0):\n",
        "        log += \" - \" + generate_sample(70, '\\n').replace(\"\\n\", \" \")\n",
        "      if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
        "        sys.stdout.write(log)\n",
        "    optim.alpha *= 0.99 # Reducing the learning rate\n",
        "    print()\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "7RZ530c4Wl0d",
        "outputId": "495ceb52-c582-4ce8-a7be-f567967a4a76"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "cannot backprop more than once",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-57b0e25957a2>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.99\u001b[0m \u001b[0;31m# Reducing the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-57b0e25957a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterations)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#   loss = loss + batch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m           \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-2966fb7da995>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# This is the complicated derivation we did in part 4 of the makemore series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cross_entropy complicated deriv backprop dx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-2966fb7da995>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0;31m# begins actual back propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-2966fb7da995>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_origin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_origin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot backprop more than once\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad_origin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: cannot backprop more than once"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "voj-TQuRaNcU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}