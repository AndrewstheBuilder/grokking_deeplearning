{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/tZc1aUUS+zCIWcfg3xl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewstheBuilder/grokking_deeplearning/blob/main/LSTM_CH14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning Framework"
      ],
      "metadata": {
        "id": "oos3EL_c9kre"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q3Ydt8SW9hu3"
      },
      "outputs": [],
      "source": [
        "# Tensor Class. The Foundation of the Deep Learning Framework\n",
        "import numpy as np\n",
        "class Tensor (object):\n",
        "  def __init__(self,data,label,\n",
        "               autograd=False,\n",
        "               creators=None,\n",
        "               creation_op=None,\n",
        "               id=None):\n",
        "    self.label = label\n",
        "    self.data = np.array(data)\n",
        "    self.creation_op = creation_op\n",
        "    self.creators = creators\n",
        "    self.grad = None\n",
        "    self.autograd = autograd\n",
        "    self.children = {}\n",
        "    if(id is None):\n",
        "      id = np.random.randint(0,100000) # What is the likelyhood of producing the same id for a tensor in the same session?\n",
        "    self.id = id\n",
        "\n",
        "    if(creators is not None):\n",
        "      for c in creators:\n",
        "        # keeps track of how many children a tensor has\n",
        "        if(self.id not in c.children):\n",
        "          # Initialize c.children[self.id]\n",
        "          # We are giving the creator the children property\n",
        "          c.children[self.id] = 1\n",
        "        else:\n",
        "          # Update counter for children\n",
        "          c.children[self.id] += 1\n",
        "\n",
        "  def all_children_grads_accounted_for(self):\n",
        "    '''\n",
        "    Checks whether a tensor has received the correct\n",
        "    number of gradients from each child\n",
        "    '''\n",
        "    for id,cnt in self.children.items():\n",
        "      if(cnt != 0):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  def backward(self,grad=None,grad_origin=None):\n",
        "    if(self.autograd):\n",
        "\n",
        "      if(grad is None):\n",
        "        grad = Tensor(np.ones_like(self.data), 'grad'+str(self.data.shape))\n",
        "\n",
        "      if(grad_origin is not None):\n",
        "        if(self.children[grad_origin.id] == 0):\n",
        "          raise Exception(\"cannot backprop more than once\")\n",
        "        else:\n",
        "          self.children[grad_origin.id] -= 1\n",
        "\n",
        "      if(self.grad is None):\n",
        "        self.grad = grad\n",
        "      else:\n",
        "        # accumulates gradients from several children\n",
        "        self.grad += grad\n",
        "\n",
        "      if(self.creators is not None and\n",
        "         (self.all_children_grads_accounted_for() or\n",
        "          grad_origin is None)):\n",
        "          # begins actual back propagation\n",
        "          if(self.creation_op == \"add\"):\n",
        "            self.creators[0].backward(grad,self)\n",
        "            self.creators[1].backward(grad,self)\n",
        "\n",
        "          if(self.creation_op == \"neg\"):\n",
        "            self.creators[0].backward(self.grad.__neg__())\n",
        "\n",
        "          if(self.creation_op == \"sub\"):\n",
        "            new = Tensor(self.grad.data, label='sub_grad')\n",
        "            self.creators[0].backward(new, self)\n",
        "            new = Tensor(self.grad.__neg__().data, label='sub_grad2')\n",
        "            self.creators[1].backward(new,self)\n",
        "\n",
        "          if(self.creation_op == \"mul\"):\n",
        "            new = self.grad * self.creators[1].data\n",
        "            self.creators[0].backward(new, self)\n",
        "            new = self.grad * self.creators[0].data\n",
        "            self.creators[1].backward(new, self)\n",
        "\n",
        "          if(self.creation_op == \"mm\"):\n",
        "            # Usually an activation\n",
        "            act = self.creators[0]\n",
        "            weights = self.creators[1]\n",
        "            new = self.grad.mm(weights.transpose())\n",
        "            act.backward(new)\n",
        "            new = self.grad.transpose().mm(act).transpose()\n",
        "            weights.backward(new)\n",
        "\n",
        "          if(self.creation_op == \"transpose\"):\n",
        "            self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "          if(\"sum\" in self.creation_op):\n",
        "            dim = int(self.creation_op.split(\"_\")[1])\n",
        "            ds = self.creators[0].data.shape[dim]\n",
        "            self.creators[0].backward(self.grad.expand(dim, ds))\n",
        "\n",
        "          if(\"expand\" in self.creation_op):\n",
        "            dim = int(self.creation_op.split(\"_\")[1])\n",
        "            self.creators[0].backward(self.grad.sum(dim))\n",
        "\n",
        "          if(self.creation_op == \"sigmoid\"):\n",
        "            ones = Tensor(np.ones_like(self.grad.data), \"ones used in sigmoid backprop\")\n",
        "            self.creators[0].backward(Tensor(self.grad.data * (self.data * (ones.data - self.data)), \"sigmoid_grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"tanh\"):\n",
        "            ones = Tensor(np.ones_like(self.grad.data))\n",
        "            self.creators[0].backward(Tensor(self.grad.data * (ones.data - self.data), \"tanh_grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"index_select\"):\n",
        "            new_grad = np.zeros_like(self.creators[0].data)\n",
        "            indices_ = self.index_select_indices.data.flatten()\n",
        "            grad_ = grad.data.reshape(len(indices_), -1)\n",
        "            for i in range(len(indices_)):\n",
        "              new_grad[indices_[i]] += grad_[i]\n",
        "            self.creators[0].backward(Tensor(new_grad, \"index_select grad2\"))\n",
        "\n",
        "          if(self.creation_op == \"cross_entropy\"):\n",
        "            # This is the complicated derivation we did in part 4 of the makemore series\n",
        "            dx = self.softmax_output - self.target_dist\n",
        "            self.creators[0].backward(Tensor(dx, \"cross_entropy complicated deriv backprop dx\"))\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data + other.data,\n",
        "                    label=self.label+' + '+other.label,\n",
        "                    autograd = True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"add\")\n",
        "    return Tensor(self.data + other.data, 'add no grad')\n",
        "\n",
        "  def __neg__(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data*-1,\n",
        "                    label='-'+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"neg\",)\n",
        "    return Tensor(self.data*-1, 'neg no grad')\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data - other.data,\n",
        "                    label = self.label + ' - ' + other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"sub\")\n",
        "    return Tensor(self.data - other.data, 'sub no grad')\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    if(self.autograd and other.autograd):\n",
        "      return Tensor(self.data * other.data,\n",
        "                    label = self.label+'*'+other.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self, other],\n",
        "                    creation_op=\"mul\")\n",
        "    return Tensor(self.data - other.data, 'mul no grad')\n",
        "\n",
        "  def sum(self, dim):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.sum(dim),\n",
        "                    label = self.label+'.sum_'+str(dim)+')',\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sum_\"+str(dim))\n",
        "    return Tensor(self.data.sum(dim), 'sum no grad')\n",
        "\n",
        "  def expand(self, dim, copies):\n",
        "\n",
        "    trans_cmd = list(range(0, len(self.data.shape)))\n",
        "    trans_cmd.insert(dim, len(self.data.shape))\n",
        "    new_shape = list(self.data.shape) + [copies]\n",
        "    new_data = self.data.repeat(copies).reshape(new_shape)\n",
        "    new_data = new_data.transpose(trans_cmd)\n",
        "\n",
        "    if(self.autograd):\n",
        "      return Tensor(new_data,\n",
        "                    label=self.label+\".expand_\"+str(dim),\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"expand_\"+str(dim))\n",
        "    return new_data\n",
        "\n",
        "  def transpose(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.transpose(),\n",
        "                    label=self.label+\".transpose\",\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"transpose\")\n",
        "    return Tensor(self.data.transpose(), \"transpose no grad\")\n",
        "\n",
        "  def mm(self,x):\n",
        "    if(self.autograd):\n",
        "      return Tensor(self.data.dot(x.data),\n",
        "                    label=self.label+\".dot_\"+x.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self,x],\n",
        "                    creation_op=\"mm\")\n",
        "    return Tensor(self.data.dot(x.data), \"mm no grad\")\n",
        "\n",
        "  # Nonlinearities\n",
        "  def sigmoid(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(1/(1+np.exp(-self.data)),\n",
        "                    label=\"sigmoid_\"+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"sigmoid\")\n",
        "    return Tensor(1/(1+np.exp(-self.data)), label=\"(no auto grad)sigmoid_\"+self.label)\n",
        "\n",
        "  def tanh(self):\n",
        "    if(self.autograd):\n",
        "      return Tensor(np.tanh(self.data),\n",
        "                    label=\"tanh_\"+self.label,\n",
        "                    autograd=True,\n",
        "                    creators=[self],\n",
        "                    creation_op=\"tanh\")\n",
        "    return Tensor(np.tanh(self.data), label=\"(no auto grad)tanh\"+self.label)\n",
        "\n",
        "  def index_select(self, indices):\n",
        "    if(self.autograd):\n",
        "      new = Tensor(self.data[indices.data],\n",
        "                   label=\"index_select w/ \"+self.label,\n",
        "                   autograd=True,\n",
        "                   creators=[self],\n",
        "                   creation_op=\"index_select\")\n",
        "      new.index_select_indices = indices\n",
        "      return new\n",
        "    return Tensor(self.data[indices.data], \"index_select no grad\")\n",
        "\n",
        "  def cross_entropy(self, target_indices):\n",
        "    temp = np.exp(self.data)\n",
        "    softmax_output = temp / np.sum(temp,\n",
        "                                   axis=len(self.data.shape)-1,\n",
        "                                   keepdims=True)\n",
        "    t = target_indices.data.flatten()\n",
        "    p = softmax_output.reshape(len(t),-1)\n",
        "    target_dist = np.eye(p.shape[1])[t]\n",
        "    loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "    if(self.autograd):\n",
        "      out = Tensor(loss,\n",
        "                   label=\"cross_entropy\",\n",
        "                   autograd=True,\n",
        "                   creators=[self],\n",
        "                   creation_op=\"cross_entropy\")\n",
        "      out.softmax_output = softmax_output\n",
        "      out.target_dist = target_dist\n",
        "      return out\n",
        "    return Tensor(loss, \"cross_entropy no grad\")\n",
        "\n",
        "  def __repr__(self):\n",
        "    # This method calls the self.data's repr method\n",
        "    return str(self.data.__repr__())\n",
        "    # return str(self.label.__repr__() + \":\" + self.data.__repr__())\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.data.__str__())\n",
        "    # return str(self.label.__repr__() + \":\" + self.data.__str__() + ' Creators:'+self.creators.__str__())\n",
        "\n",
        "# Other Classes for the Deep Learning (DL) Framework\n",
        "class SGD(object):\n",
        "  def __init__(self, parameters, alpha=0.1):\n",
        "    self.parameters = parameters\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def zero(self):\n",
        "    # zero parameters' gradients\n",
        "    for p in self.parameters:\n",
        "      p.grad.data *= 0\n",
        "\n",
        "  def step(self, zero=True):\n",
        "    # update parameters' data based on their gradients\n",
        "    # zero out the gradient after if zero=True\n",
        "    for p in self.parameters:\n",
        "      p.data -= p.grad.data * self.alpha\n",
        "      if(zero):\n",
        "        p.grad.data *= 0\n",
        "\n",
        "# The Layer Class another foundation for the DL Framework\n",
        "class Layer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.parameters = list()\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return self.parameters\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    super().__init__()\n",
        "    W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs))\n",
        "    self.weight = Tensor(W, autograd=True, label='Linear W')\n",
        "    self.bias = Tensor(np.zeros(n_outputs), autograd=True, label='Linear b')\n",
        "\n",
        "    self.parameters.append(self.weight)\n",
        "    self.parameters.append(self.bias)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.mm(self.weight)+self.bias.expand(0, len(input.data))\n",
        "\n",
        "class Sequential(Layer):\n",
        "\n",
        "  def __init__(self, layers=list()):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = layers\n",
        "\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def forward(self, input):\n",
        "    for layer in self.layers:\n",
        "      input = layer.forward(input)\n",
        "    return input\n",
        "\n",
        "  def get_parameters(self):\n",
        "    params = list()\n",
        "    for l in self.layers:\n",
        "      params += l.get_parameters()\n",
        "    return params\n",
        "\n",
        "class Tanh(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.tanh()\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    return input.sigmoid()\n",
        "\n",
        "class Embedding(Layer):\n",
        "\n",
        "  def __init__(self, vocab_size, dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dim = dim\n",
        "\n",
        "    # this initialization style is a convention from word2vec\n",
        "    weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
        "    self.weight = Tensor(weight, label=\"Embedding weight\", autograd=True)\n",
        "\n",
        "    self.parameters.append(self.weight)\n",
        "\n",
        "  def forward(self, input):\n",
        "    return self.weight.index_select(input)\n",
        "\n",
        "# You can also create layers that are functions on the input\n",
        "class MSELoss(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, pred, target):\n",
        "    return ((pred-target)*(pred-target)).sum(0)\n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    return input.cross_entropy(target)\n",
        "\n",
        "class RNNCell(Layer):\n",
        "  def __init__(self,n_inputs,n_hidden,n_output,activation='sigmoid'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.n_inputs = n_inputs\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_output = n_output\n",
        "\n",
        "    if(activation == 'sigmoid'):\n",
        "      self.activation = Sigmoid()\n",
        "    elif(activation == 'tanh'):\n",
        "      self.activation = Tanh()\n",
        "    else:\n",
        "      raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "    self.w_ih = Linear(n_inputs, n_hidden)\n",
        "    self.w_hh = Linear(n_hidden, n_hidden)\n",
        "    self.w_ho = Linear(n_hidden, n_output)\n",
        "\n",
        "    self.parameters += self.w_ih.get_parameters()\n",
        "    self.parameters += self.w_hh.get_parameters()\n",
        "    self.parameters += self.w_ho.get_parameters()\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "      from_prev_hidden = self.w_hh.forward(hidden)\n",
        "      combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "      new_hidden = self.activation.forward(combined)\n",
        "      output = self.w_ho.forward(new_hidden)\n",
        "      return output, new_hidden\n",
        "\n",
        "  def init_hidden(self, batch_size=1):\n",
        "    # What is this used for?\n",
        "    return Tensor(np.zeros((batch_size, self.n_hidden)), label=\"RNN Hidden State\", autograd=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "PlYNhqid_Eoe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sigmoid,relu) = (lambda x: 1/1+np.exp(-x)), lambda x: (x>0).astype(float)*x\n",
        "weights = np.array([[1,4],[4,1]])\n",
        "activation = sigmoid(np.array([1, 0.01]))\n",
        "\n",
        "print(\"Sigmoid Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  activation = sigmoid(activation.dot(weights))\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\nSigmoid Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  # The derivative of sigmoid causes very small gradients when activation is very near 0 or 1\n",
        "  sigmoid_deriv = (activation) * (1-activation)\n",
        "   # Chain Rule\n",
        "  gradient = sigmoid_deriv * gradient\n",
        "  gradient = gradient.dot(weights.transpose()) # So this is also part of the chain rule???\n",
        "  print(gradient)\n",
        "\n",
        "print(\"\\nRelu Activations\")\n",
        "activations = list()\n",
        "for iter in range(10):\n",
        "  # The matrix multiplication causes exploding gradients that don't get squashed by a nonlinearity as in sigmoid\n",
        "  activation = relu(activation.dot(weights))\n",
        "\n",
        "  activations.append(activation)\n",
        "  print(activation)\n",
        "print(\"\\n Relu Gradients\")\n",
        "gradient = np.ones_like(activation)\n",
        "for activation in reversed(activations):\n",
        "  gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
        "  print(gradient)\n",
        "\n",
        "# Adding gates to RNN will replace all of the nonlinearies and matrix multiplications"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlBgOmQ0_BjK",
        "outputId": "88de935d-9b2a-47fa-b78d-e86956a89fc3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Activations\n",
            "[1.00008889 1.00057475]\n",
            "[1.00672188 1.00673168]\n",
            "[1.006515   1.00651519]\n",
            "[1.00652199 1.00652199]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "[1.00652177 1.00652177]\n",
            "\n",
            "Sigmoid Gradients\n",
            "[-0.03282154 -0.03282154]\n",
            "[0.00107725 0.00107725]\n",
            "[-3.53571093e-05 -3.53571093e-05]\n",
            "[1.16047468e-06 1.16047468e-06]\n",
            "[-3.80885641e-08 -3.80885641e-08]\n",
            "[1.25012386e-09 1.25012385e-09]\n",
            "[-4.10323732e-11 -4.10323591e-11]\n",
            "[1.34536847e-12 1.34534485e-12]\n",
            "[-4.55737812e-14 -4.55341556e-14]\n",
            "[1.08795551e-16 4.23921767e-17]\n",
            "\n",
            "Relu Activations\n",
            "[5.00238791 5.00093033]\n",
            "[25.00610921 25.01048197]\n",
            "[125.04803709 125.03491883]\n",
            "[625.1877124  625.22706719]\n",
            "[3126.09598115 3125.97791678]\n",
            "[15630.00764826 15630.36184138]\n",
            "[78151.45501378 78150.39243441]\n",
            "[390753.02475143 390756.21248955]\n",
            "[1953777.87470964 1953768.31149529]\n",
            "[9768851.12069078 9768879.81033384]\n",
            "\n",
            " Relu Gradients\n",
            "[5. 5.]\n",
            "[25. 25.]\n",
            "[125. 125.]\n",
            "[625. 625.]\n",
            "[3125. 3125.]\n",
            "[15625. 15625.]\n",
            "[78125. 78125.]\n",
            "[390625. 390625.]\n",
            "[1953125. 1953125.]\n",
            "[9765625. 9765625.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM Implementation CH 14"
      ],
      "metadata": {
        "id": "iSScP21c_eyb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pHyT7kJO_eOO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}